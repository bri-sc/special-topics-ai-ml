{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Your Own\n",
    "\n",
    "Practically, writing a complex artificial neural network from scratch is inefficient.\n",
    "This is particularly the case when we are working in Python, which has a broad range of tooling that covers neural network machine learning. \n",
    "Instead, we will look at using `pytorch` to create our neural network. \n",
    "We will start by importing `pytorch`. \n",
    "````{margin}\n",
    "```{note}\n",
    "This specific example was taken from the `pytorch` [quick start documentation](https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html) as an example.\n",
    "```\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "\n",
    "For this example, we will use the FashionMNIST dataset, which has 70 000 examples of images of clothes. \n",
    "Each of the images has been labelled with the type of clothing. \n",
    "Below, we list all of the classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\n",
    "    \"T-shirt/top\",\n",
    "    \"Trouser\",\n",
    "    \"Pullover\",\n",
    "    \"Dress\",\n",
    "    \"Coat\",\n",
    "    \"Sandal\",\n",
    "    \"Shirt\",\n",
    "    \"Sneaker\",\n",
    "    \"Bag\",\n",
    "    \"Ankle boot\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is split into a train and a test dataset; we will download these datasets from the `torchvision`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from IPython.utils import io\n",
    "\n",
    "with io.capture_output() as captured:\n",
    "    training_data = datasets.FashionMNIST(\n",
    "        root=\"../data\",\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=ToTensor(),\n",
    "    )\n",
    "\n",
    "    test_data = datasets.FashionMNIST(\n",
    "        root=\"../data\",\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=ToTensor(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is then passed to a data loader. \n",
    "This object will dynamically produce different data from the base dataset for each training period. \n",
    "This data is loaded in a batching fashion, so only some given dataset size is returned each time the data loader is called. \n",
    "Here, we use a batch size of 64. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building A Network\n",
    "\n",
    "A neural network in `pytorch` is defined by creating a subclass of the `nn.Module`. \n",
    "The layers are then defined, and the forward propagation is defined. \n",
    "````{margin}\n",
    "```{note}\n",
    "You should investigate the appropriate [documentation](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear) to understand what the `nn.Linear` layer is doing. \n",
    "```\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "    \n",
    "model = NeuralNetwork()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `nn.ReLU` is a [rectified linear unit activation function](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)). \n",
    "This non-linear activation function is commonly found in deep neural networks. \n",
    "\n",
    "## Model Optimisation \n",
    "\n",
    "The last two parts, which we should be familiar with now, are the loss function and the optimiser. \n",
    "Below, we use slightly more advanced approaches than when we wrote our own, but the `SGD` optimiser is still a gradient descent approach. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimiser = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the components are now in place to build the training and testing functions. \n",
    "Here, the data are fed into the training in batches, and backpropagation is used to adjust the model parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimiser):\n",
    "    \"\"\" \n",
    "    Trains the model\n",
    "    \n",
    "    :param dataloader: DataLoader object\n",
    "    :param model: Neural network model\n",
    "    :param loss_fn: Loss function\n",
    "    :param optimiser: Optimiser\n",
    "    \"\"\"\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        optimiser.zero_grad()\n",
    "        loss.backward()\n",
    "        optimiser.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    \"\"\"\n",
    "    Tests the model\n",
    "    \n",
    "    :param dataloader: DataLoader object\n",
    "    :param model: Neural network model\n",
    "    :param loss_fn: Loss function\n",
    "    \"\"\"\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````{margin}\n",
    "```{note}\n",
    "Note that the meaning of epoch has changed slightly.\n",
    "```\n",
    "````\n",
    "Here, the iteration of our epochs is performed. \n",
    "In an epoch, the model is trained to make better predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimiser)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that after 10 epochs, the model could predict the clothing object with 70 % accuracy. \n",
    "Let's put that to the test. \n",
    "\n",
    "## Visualisation of the Model\n",
    "\n",
    "Lets randomly select four examples from the training data and see how the network does. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "rng = np.random.RandomState(42)\n",
    "samples = rng.randint(0, test_data.data.shape[0], 4)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "fig, ax = plt.subplots(2, 2, figsize=(10, 10))\n",
    "ax = ax.flatten()\n",
    "for i, sample in enumerate(samples):\n",
    "    with torch.no_grad():\n",
    "        pred = model(test_data.data[sample].float().view(1, -1))\n",
    "        ax[i].imshow(test_data.data[sample], cmap='gray')\n",
    "        ax[i].set_title(f\"Prediction: {classes[pred.argmax(1).item()]}, Actual: {classes[test_data.targets[sample].item()]}\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "special-topics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
