{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a CNN\n",
    "\n",
    "We have discussed the different components of a convolutional neural network; now, we can bring them all together.\n",
    "For this, we will return to the FashionMNIST dataset we saw [previously](./build-your-own.ipynb). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "from IPython.utils import io\n",
    "\n",
    "with io.capture_output() as captured:\n",
    "    training_data = datasets.FashionMNIST(\n",
    "        root=\"../data\",\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=ToTensor(),\n",
    "    )\n",
    "\n",
    "    test_data = datasets.FashionMNIST(\n",
    "        root=\"../data\",\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=ToTensor(),\n",
    "    )\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the data loaded, the next step is to create the model. \n",
    "This model consists of three convolutional layers, each using a ReLU activation function, and is followed by a maximum pooling. \n",
    "Following the final maximum pooling, the data is flattened to be passed to the linear, fully connected layers (of which there are four). \n",
    "The final fully connected layer produces 10 output classes, matching the 10 types of clothing in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNNModel(nn.Module):\n",
    "    \"\"\"\n",
    "    A convolutional neural network model for image classification.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=0)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=0)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=0)\n",
    "        \n",
    "        # Pooling layer\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(64 * 3 * 3, 250)  # Adjusted for 28x28 input after convolution and pooling\n",
    "        self.fc2 = nn.Linear(250, 125)\n",
    "        self.fc3 = nn.Linear(125, 60)\n",
    "        self.fc4 = nn.Linear(60, 10)  # 10 classes for classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the neural network.\n",
    "        \n",
    "        :param x: The input tensor\n",
    "        :return: The output tensor\n",
    "        \"\"\"\n",
    "        # Outputs: 32 × 26 × 26 → 32 × 13 × 13\n",
    "        x = self.pool(F.relu(self.conv1(x)))  \n",
    "        # Output: 64 × 11 × 11 → 64 × 5 × 5\n",
    "        x = self.pool(F.relu(self.conv2(x)))  \n",
    "        # Output: 64 × 3 × 3\n",
    "        x = F.relu(self.conv3(x))             \n",
    "\n",
    "        # Flattens for fully connected layers to (batch_size, 64 * 3 *3)\n",
    "        x = torch.flatten(x, start_dim=1) \n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        # No activation, as CrossEntropyLoss in PyTorch expects raw logits\n",
    "        # x = self.fc4(x)  \n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the simple model built, we can create the object and print what the model looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNNModel()\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define our loss function (here, we use cross-entropy loss as we are working with a classification problem) and the optimiser. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything is now ready to train the network for 10 epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, criterion, optimizer, epochs=10):\n",
    "    model.train() \n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            optimizer.zero_grad()  \n",
    "            outputs = model(images)  \n",
    "            loss = loss_fn(outputs, labels)  \n",
    "            loss.backward()  \n",
    "            optimizer.step() \n",
    "\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader):.4f}\")\n",
    "\n",
    "train_model(model, train_dataloader, loss_fn, optimizer, epochs=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, with the model trained, we can look at how well it handles the test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "model.eval()  # Set model to evaluation mode\n",
    "f1_scores = []\n",
    "accuracy_scores = []\n",
    "\n",
    "with torch.no_grad():  # No need to compute gradients during evaluation\n",
    "    for images, labels in test_dataloader:\n",
    "\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)  # Get class with highest probability\n",
    "\n",
    "        f1_scores.append(f1_score(labels, predicted, average='macro'))\n",
    "        accuracy_scores.append(accuracy_score(labels, predicted))\n",
    "\n",
    "print(f\"Average Test Accuracy: {np.mean(accuracy_scores):.2f}%\")\n",
    "print(f\"Average Test F1 Score: {np.mean(f1_scores):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average accuracy and F1 score are high, indicating that even for this simple network, the model can classify the images better than our [simple linear network](./build-your-own.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "special-topics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
