{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks\n",
    "\n",
    "The neural networks we have investigated are sometimes called feedforward neural networks. \n",
    "These types of neural networks can be limited as the inputs are all processed independently by the network. \n",
    "The independence is addressed with recurrent neural networks, where the output of a neuron for a given data point is fed in as an input for the following data point. \n",
    "This makes recurrent neural networks uniquely suited for modelling sequential data, such as text, speech and time series. \n",
    "As a result, recurrent neural networks have significantly influenced the transformer networks that have led to the revolutionary large language models. \n",
    "\n",
    "We will use a simple time series dataset for the electrical production to put together a simple recurrent neural network. \n",
    "You can [download the dataset here](../data/electric.csv).\n",
    "````{margin}\n",
    "```{note}\n",
    "Here, we also convert the `'date'` column to a `datetime` object.\n",
    "```\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "timeseries = pd.read_csv('../data/electric.csv')\n",
    "timeseries['date'] = pd.to_datetime(timeseries['date'])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "timeseries.plot(x='date', y='elec-prod', ax=ax)\n",
    "ax.tick_params(axis='x', labelrotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedback\n",
    "\n",
    "As mentioned above, the difference between a traditional neural network and a recurrent neural network is that the latter includes the hidden state of the previous data point in the activation function. \n",
    "That means that for some activation function, $f$, the traditional network as the equation, \n",
    "\n",
    "$$\n",
    "h_i = f(W_xx_i + b_i),\n",
    "$$\n",
    "\n",
    "while for a recurrent network, we add the previous hidden state, \n",
    "\n",
    "$$\n",
    "h_i = f(W_hh_{i-1} + W_xx_i + b).\n",
    "$$\n",
    "\n",
    "We implement a simple feedback perceptron with Python below, using a [tanh](https://numpy.org/doc/2.0/reference/generated/numpy.tanh.html) activation function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SimpleRNNPerceptron:\n",
    "    \"\"\"\n",
    "    A simple RNN perceptron with a single hidden layer.\n",
    "    \n",
    "    :param input_size: The size of the input vector\n",
    "    :param hidden_size: The size of the hidden layer\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.W_x = np.random.randn(hidden_size, input_size) * 0.1\n",
    "        self.W_h = np.random.randn(hidden_size, hidden_size) * 0.1\n",
    "        self.b = np.zeros((hidden_size, 1))\n",
    "        self.h_i = np.zeros((hidden_size, 1)) \n",
    "\n",
    "    def step(self, x_i):\n",
    "        \"\"\"\n",
    "        Processes a single step\n",
    "\n",
    "        :param x_i: The input vector\n",
    "        :return: The output vector\n",
    "        \"\"\"\n",
    "        x_i = x_i.reshape(-1, 1) \n",
    "        self.h_i = np.tanh(np.dot(self.W_x, x_i) + np.dot(self.W_h, self.h_i) + self.b)\n",
    "        return self.h_i\n",
    "\n",
    "rnn = SimpleRNNPerceptron(input_size=1, hidden_size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can run this through a single year of our time series data, where the hidden size is 5. \n",
    "The hidden size is the number of neurons in the hidden layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries_2017 = timeseries['elec-prod'][timeseries['date'].dt.year == 2017]\n",
    "timeseries_2017 /= timeseries_2017.max()\n",
    "\n",
    "for i, x_i in enumerate(timeseries_2017):\n",
    "    h_i = rnn.step(np.array([x_i])) \n",
    "    print(f\"Step {i+1}: Hidden State = {h_i.ravel()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The larger the hidden size, the more capacity the network has to learn complex patterns, which comes with a computational cost. \n",
    "\n",
    "## Implementation with `pytorch`\n",
    "\n",
    "The Python library `pytorch` comes with an implementation of [recurrent neural networks](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html). \n",
    "We advise you to study the documentation to understand how a recurrent neural network may be implemented using `pytorch`. \n",
    "Furthermore, you should appreciate the configurability of this `nn.Module` object. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "special-topics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
