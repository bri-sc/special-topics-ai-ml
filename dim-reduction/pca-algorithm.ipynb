{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Does PCA Work? \n",
    "\n",
    "### Data Cleaning and Scaling\n",
    "\n",
    "Before we look at the actual algorithm, we will read in the abalone data, discussed previously, and *clean* it for processing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('./../data/abalone.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned, the `sex` data is catagorical, and therefore incompatible with the PCA algorithm, in the form that we will discuss. \n",
    "So, we should remove that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned = data[data.columns[1:]]\n",
    "cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The remaining features are continuous variables and therefore can be used for the PCA algorithm. \n",
    "```{warning}\n",
    "Here, we highlight that the `rings` data is not strictly continuous, as it can only be integer values.\n",
    "We refer to data of this type as **discrete**. \n",
    "However, as it does not describe a *class* of the feature, like `sex`, it is compatible with PCA. \n",
    "```\n",
    "\n",
    "If we have a look at the variances for each feature, we will see that some features (`rings`) cover much larger ranges than others (`diameter`). \n",
    "Therefore, if we na√Øvely applied the PCA algorithm, we risk the first principal component containing only really information about the `rings`, as these have the most variance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned.var()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, we must scale our data. \n",
    "There are a few approaches to this, but here we will scale our data such that the mean for each feature is 0 and the variance is 1. \n",
    "In `scikit-learn` this is called the [`StandardScaler()`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler), but here we will do it manually to show how it is achieved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled = (cleaned - cleaned.mean()) / cleaned.std()\n",
    "scaled.var()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data is now ready for the PCA algorithm. \n",
    "\n",
    "### The PCA Algorithm\n",
    "\n",
    "Strictly, speaking the PCA algorithm involves the calculation of the covariance matrix for the data, followed by the determination of the eigenvalues and eigenvectors.\n",
    "The eigenvectors are the principal components, which can be sorted by their eigenvalues, which are the explained variance. \n",
    "We can see this in action for our data below. \n",
    "````{margin}\n",
    "```{note}\n",
    "The `np.argsort` function returns the indices of of the input sorted numerically increasing. \n",
    "Therefore, we flip these indices as we want the largest eigenvalue first. \n",
    "```\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "cov = scaled.cov()\n",
    "eigenthings = np.linalg.eig(cov)\n",
    "explained_variance = eigenthings.eigenvalues\n",
    "components = eigenthings.eigenvectors[:, np.argsort(explained_variance)[::-1]].T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the covariance matrix from our input data after the linear transformation by the matrix that our PCA algorithm defines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed = components / np.sqrt(explained_variance[:, np.newaxis]) @ scaled.T\n",
    "scaled_with_pc = scaled.copy()\n",
    "for i in range(8):\n",
    "    scaled_with_pc[f'pc{i+1}'] = transformed.loc[i]\n",
    "scaled_with_pc[[f'pc{i+1}' for i in range(cleaned.shape[1])]].cov()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may notice, that something has gone wrong in our analysis, for example, looking at the variance of `pc6`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_with_pc['pc6'].var()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This variance is significantly larger than the expected 1. \n",
    "The problem we are facing here isn't in our algorithm, but rather the numerical implementation. \n",
    "\n",
    "````{margin}\n",
    "```{note}\n",
    "The SVD approach to PCA is more flexible by allowing \"truncated PCA\", where not all components are computed, but just the top *n*. \n",
    "```\n",
    "````\n",
    "The approach of calculating the covariance matrix, followed by computing the eigenvalues and eigenvectors can lead to the numerical instability we see above. \n",
    "Furthermore, the computationl of the covariance matrix is often slow, where there are a large number of datapoints. \n",
    "Therefore, a more stable (and flexible) approach is to use singular value decomposition (this is how `scikit-learn` does it). \n",
    "\n",
    "The SVD approach does not require the covariance matrix to be computed, instead the SVD is performed directly on the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, S, Vt = np.linalg.svd(scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The explained variance is then the square of the singular values, scaled by the number of samples in the data (minus one, as it is a sample variance). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explained_variance = np.square(S) / (scaled.shape[0] - 1)\n",
    "explained_variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The components are then the $\\mathbf{V}^\\top$ columns, which are already sorted in terms of decreasing singular values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components = Vt\n",
    "\n",
    "transformed = components / np.sqrt(explained_variance[:, np.newaxis]) @ scaled.T\n",
    "for i in range(8):\n",
    "    scaled_with_pc[f'pc{i+1}'] = transformed.loc[i]\n",
    "scaled_with_pc[[f'pc{i+1}' for i in range(cleaned.shape[1])]].cov()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this implementation does not have the limitations of the previous approach, with an approximate identity matrix being found. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "special-topics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
