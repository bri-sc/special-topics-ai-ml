{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How Does PCA Work? \n",
    "\n",
    "## Data Cleaning and Scaling\n",
    "\n",
    "Before we look at the algorithm, we will read in the abalone data [discussed previously](./setup/datasets.html) and *clean* it for processing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('./../data/abalone.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned, the `sex` data is categorical and, therefore, incompatible with the PCA algorithm in the form that we will discuss. \n",
    "So, we should remove that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned = data[data.columns[1:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The remaining features are continuous variables and can be used for the PCA algorithm. \n",
    "```{warning}\n",
    "Here, we highlight that the `rings` data is not strictly continuous, as it can only be integer values.\n",
    "We refer to data of this type as **discrete**. \n",
    "However, as it does not describe a *class* of the feature, like `sex`, it is compatible with PCA. \n",
    "```\n",
    "\n",
    "If we look at the variances for each feature, we will see that some features (`rings`) cover much larger ranges than others (`diameter`). \n",
    "Therefore, if we na√Øvely applied the PCA algorithm, we risk the first principal component containing only really information about the `rings`, as these have the most variance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned.var()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, we must scale our data. \n",
    "There are a few approaches to this, but here, we will scale our data such that the mean for each feature is 0 and the variance is 1. \n",
    "In `scikit-learn` this is called the [`StandardScaler()`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler), but here we will do it manually to show how it is achieved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled = (cleaned - cleaned.mean()) / cleaned.std()\n",
    "scaled.var()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data is now ready for the PCA algorithm. \n",
    "\n",
    "## The PCA Algorithm\n",
    "\n",
    "Strictly speaking, the PCA algorithm involves the calculation of the covariance matrix for the data, followed by determining the eigenvalues and eigenvectors.\n",
    "The eigenvectors are the principal components, which can be sorted by their eigenvalues, which are the explained variance. \n",
    "We can see this in action for our data below. \n",
    "````{margin}\n",
    "```{note}\n",
    "The `np.argsort` function returns the indices of the input sorted numerically increasing. \n",
    "Therefore, we flip these indices as we want the largest eigenvalue first. \n",
    "```\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "cov = scaled.cov()\n",
    "eigenthings = np.linalg.eig(cov)\n",
    "indices = np.argsort(eigenthings.eigenvalues)[::-1]\n",
    "explained_variance = eigenthings.eigenvalues[indices]\n",
    "components = eigenthings.eigenvectors[:, indices].T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the covariance matrix from our input data after the linear transformation by the matrix that our PCA algorithm defines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed = components / np.sqrt(explained_variance[:, np.newaxis]) @ scaled.T\n",
    "scaled_with_pc = scaled.copy()\n",
    "for i in range(8):\n",
    "    scaled_with_pc[f'PC{i+1}'] = transformed.loc[i]\n",
    "scaled_with_pc[[f'PC{i+1}' for i in range(cleaned.shape[1])]].cov()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this is a diagonal matrix, as we would expect. \n",
    "````{margin}\n",
    "```{note}\n",
    "The SVD approach to PCA is more flexible by allowing \"truncated PCA\", where not all components are computed, but just the top *n*. \n",
    "```\n",
    "````\n",
    "\n",
    "However, calculating the covariance matrix can be slow, where there are a large number of data points. \n",
    "Therefore, a more stable (and flexible) approach is to use singular value decomposition (this is how `scikit-learn` does it). \n",
    "\n",
    "The SVD approach does not require the computation of the covariance matrix. \n",
    "Instead, the SVD is performed directly on the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, S, Vt = np.linalg.svd(scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The explained variance is then the square of the singular values, scaled by the number of samples in the data (minus one, as it is a sample variance). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explained_variance = np.square(S) / (scaled.shape[0] - 1)\n",
    "explained_variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The components are then the $\\mathbf{V}^\\top$ columns, which are already sorted in terms of decreasing singular values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components = Vt\n",
    "\n",
    "transformed = components / np.sqrt(explained_variance[:, np.newaxis]) @ scaled.T\n",
    "for i in range(8):\n",
    "    scaled_with_pc[f'PC{i+1}'] = transformed.loc[i]\n",
    "scaled_with_pc[[f'PC{i+1}' for i in range(cleaned.shape[1])]].cov()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "special-topics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
