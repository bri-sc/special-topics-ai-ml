{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# t-SNE\n",
    "\n",
    "t-distributed stochastic neighbour embedding or t-SNE is another dimensionality reduction algorithm. \n",
    "The use case for t-SNE is different from PCA, in that it is strongly focused on the visualisation of data and the interpretability of the t-SNE model is usually very low. \n",
    "However, the nature of the t-SNE algorithm makes it more suitable for non-linear data than PCA. \n",
    "Similar to PCA, t-SNE is often used as a tool to reduce the dimensionality of some data, before a clustering algorithm is applied. \n",
    "\n",
    "```{figure} ../images/tsne_mnist.png\n",
    "---\n",
    "name: tsne-mnist\n",
    "width: 100%\n",
    "---\n",
    "The result of a t-SNE analysis of 7000 samples from the MNIST handwritten digits dataset.\n",
    "```\n",
    "\n",
    "````{margin}\n",
    "```{note}\n",
    "The result of this algorithm will likely give different results to the `scikit-learn` implementation. \n",
    "This is due to differences in how the optimisation of the KL divergence is performed. \n",
    "```\n",
    "````\n",
    "## t-SNE Algorithm\n",
    "\n",
    "The t-SNE algorithm converts the distances between data points in the original space into probabilities. \n",
    "These probabilities represent the similarity between the data points. \n",
    "Then in the lower dimensional description of the data, we aim to describe the data, following the same probability distributions. \n",
    "This is achieved by minimizing the Kullback-Leibler (KL) divergence between the low-dimensional data and original data probability distributions. \n",
    "\n",
    "Let's have a look at how that looks in code. \n",
    "For this example, we will apply it to a simple example dataset with two overlapping clusters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('./../data/tsne-example.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualise this dataset with seaborn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "g = sns.PairGrid(data=data, hue='label')\n",
    "g.map_upper(sns.scatterplot, s=15)\n",
    "g.map_lower(sns.kdeplot)\n",
    "g.map_diag(sns.kdeplot, lw=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear for this dataset that there are two \"classes\" of data, indeed the input file includes a label to help us see them visually. \n",
    "However, the spacial distribution of the two classes is a particular set that PCA is known to be poor at handling, look, in particular, at the cross plot between `feature0` and `feature1`. \n",
    "Therefore, this is an interesting problem to see if t-SNE can improve over PCA.\n",
    "\n",
    "### Compute Similarities in Original Space\n",
    "\n",
    "The first step in the t-SNE algorithm is to compute the similarities of the data points in the original space. \n",
    "This similarity is described with the probability that datapoint $i$ would pick datapoint $j$ as it neighbour in the original space. \n",
    "These probabilities are computed from a normal distribution centred around each point and normalised. \n",
    "Mathematically, this probability is computed as: \n",
    "\n",
    "$$\n",
    "p_{j|i} = \\frac{\\exp{\\left(\\frac{-|x_i - x_j|^2}{2\\sigma_i^2}\\right)}}{\\sum_{i\\neq j}\\exp{\\left(\\frac{-|x_i - x_j|^2}{2\\sigma_i^2}\\right)}},\n",
    "$$ (affinity)\n",
    "\n",
    "where $|x_i - x_j|$ is the length of the vector between datapoints $i$ and $j$ and $\\sigma_i$ is determined for each observation $i$ using a grid search based on the hyperparameter known as **perplexity**. \n",
    "\n",
    "The first thing we will do to compute Eqn. {eq}`affinity` is find a matrix of the vector magnitudes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "diff = data.values - data.values[:, np.newaxis]\n",
    "norm = np.linalg.norm(diff, axis=-1)\n",
    "np.fill_diagonal(norm, np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the diagonal of this matrix to be `NaN` as we are concerned with pairwise similarity, not and the diagonal represents a datapoints similarity with itself. \n",
    "From this matrix, we compute $\\sigma_i$, which we achieve using a grid search across a range of values, defined by the standard deviation in the vector length.\n",
    "````{margin}\n",
    "```{note}\n",
    "Note, here the `np.nanstd` function is used to find the standard deviation and ignore the `NaN` values.\n",
    "We will continue to use `nan*` forms of NumPy functions throughout. \n",
    "```\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_norm = np.nanstd(norm, axis=1)\n",
    "sigma_search = np.linspace(0.01 * std_norm, 5 * std_norm, 200).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then calculate the probability, $p_{j|i}(\\sigma_i)$, for every value of $\\sigma_i$ in the grid search. \n",
    "Note, that we ensure that the matrix of probabilities has no zeroes, instead these are replaced with the smallest positive floating point number your computer can describe, see documentation for the [`np.nextafter`](https://numpy.org/doc/stable/reference/generated/numpy.nextafter.html) function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = np.exp(-(norm[:, :, np.newaxis] ** 2) / (2 * sigma_search ** 2))\n",
    "epsilon = np.nextafter(0, 1)\n",
    "p_norm = np.maximum(p / np.nansum(p, axis=0), epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is then to compute the Shannon entropy for each value datapoint at each value of $\\sigma_i$ in the grid search, \n",
    "\n",
    "$$\n",
    "H[p_{j|i}(\\sigma_i)] = - \\sum_j p_{j|i}(\\sigma_i) \\log_2{[p_{j|i}(\\sigma_i)]}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = -np.nansum(p_norm * np.log2(p_norm), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the ideal value for $\\sigma_i$ for each data point, $i$, we try to find the value of $\\sigma$ that results in a probability distribution with a perplexity that is as close as possible to the user-defined perplexity hyperparameter.\n",
    "The perplexity influences how the algorithm balances local and global structure in the data, with low perplexity leading to a focus on the closest individuals. \n",
    "Perplexity is conceptually related to the entropy of a probability distribution. It is defined as:\n",
    "\n",
    "$$\n",
    "\\textrm{perplexity}(\\sigma_i) = 2^{H[p_{j|i}(\\sigma_i)]} \\implies \\frac{\\textrm{perplexity}}{2^{H[p_{j|i}(\\sigma_i)]}} = 1.\n",
    "$$ \n",
    "\n",
    "Therefore, if we take the logarithm of both sides, \n",
    "\n",
    "$$\n",
    "\\ln[\\textrm{perplexity}(\\sigma_i)] - H[p_{j|i}(\\sigma_i)] + \\ln(2) = 0.\n",
    "$$\n",
    "\n",
    "We will show this for a perplexity value of 9. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexity = 9\n",
    "\n",
    "compare = np.abs(np.log(perplexity) - H + np.log(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best $\\sigma_i$ for each datapoint is then the `sigma_search` values where `compare` is minimised. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = sigma_search[range(sigma_search.shape[0]), compare.argmin(1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These sigma values are then used to calculate the similarities between the high dimensional datapoints. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_ij = np.exp(-(norm ** 2) / (2 * sigma[:, np.newaxis] ** 2))\n",
    "np.fill_diagonal(p_ij, 0)\n",
    "p_ij = np.maximum(p_ij / np.nansum(p_ij, axis=1)[:, np.newaxis], epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualise this probability matrix with the `imshow` function from `matplotlib`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "_ = ax.imshow(p_ij)\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "fig.colorbar(_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing that you may note, is that this matrix is not symmetrical.\n",
    "As we currently are computing the conditional probabilities, $p_{j|i}$, not the joint probability, $p_{ij}$. \n",
    "We can convert to the joint probabilties with the following, \n",
    "\n",
    "$$\n",
    "p_{ij} = \\frac{p_{j|i} + p_{i|j}}{2n},\n",
    "$$\n",
    "\n",
    "where $n$ is the number of datapoints. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_ij_symmetric = (p_ij + p_ij.T) / (2 * p_ij.shape[0])\n",
    "p_ij_symmetric = np.maximum(p_ij_symmetric, epsilon)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "_ = ax.imshow(p_ij_symmetric)\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "fig.colorbar(_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Initial Guesses for Lower Dimensionality Datapoints\n",
    "\n",
    "Now that we have the affinities of the high dimensional data, we can create some initial guess for the lower dimensional data. \n",
    "We want to end up with data in two dimensions.\n",
    "One of the most popular ways to produce this initial guess is to use singular value decomposition. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dimensions = 2\n",
    "\n",
    "U, S, Vt = np.linalg.svd(data)\n",
    "pca = data @ Vt.T[:, :n_dimensions]\n",
    "pca.columns = ['PC1', 'PC2']\n",
    "pca['label'] = data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(data=pca, x='PC1', y='PC2', hue='label', kind='scatter')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsurprisingly, the SVD, i.e., a PCA-approach, does not appear to be effectively in disambiguating the data. \n",
    "Let's see if t-SNE can improve on this but using the PCA results as the starting configuration. \n",
    "\n",
    "### Minimising the Kullback-Leibler (KL) Divergence\n",
    "\n",
    "As mentioned above, t-SNE minimises the KL divergence of affinities between the different spaces. \n",
    "Therefore, we need to be able to compute the affinity of the lower dimensional space (we will use $y$ for the lower dimension data). \n",
    "This is acheived by considering a Student's t-distribution (hence the t in t-SNE) with one degree of freedom, \n",
    "\n",
    "$$\n",
    "q_{ij} = \\frac{(1 + |y_i - y_j| ^ 2) ^{-1}}{\\sum_{i\\neq l}(1 + | y_i - y_l | ^ 2) ^{-1}}.\n",
    "$$\n",
    "\n",
    "Since this is something that will change as for minimise the KL divergence, as new lower-dimensional positions are generated, we will put this in a function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def low_dimensional_affinity(y):\n",
    "    \"\"\"\n",
    "    Calculates the low dimensional affinity matrix for t-SNE.\n",
    "    \n",
    "    :param y: (np.ndarray) should be a flattened array of the low dimensional data. \n",
    "    \n",
    "    :returns qq_ij: (np.ndarray) the low dimensional affinity matrix.\n",
    "    \"\"\"\n",
    "    y = y.reshape(-1, n_dimensions)\n",
    "    diff = y - y[:, np.newaxis]\n",
    "    norm = np.linalg.norm(diff, axis=-1)\n",
    "\n",
    "    qq_ij = (1 + norm ** 2) ** (-1)\n",
    "    np.fill_diagonal(qq_ij, 0)\n",
    "    qq_ij = qq_ij / qq_ij.sum()\n",
    "\n",
    "    epsilon = np.nextafter(0, 1)\n",
    "    qq_ij = np.maximum(qq_ij, epsilon)\n",
    "    return qq_ij"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function we are then looking to minimise, using `scipy.optimize.minimize`, is then as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "from scipy.special import kl_div\n",
    "\n",
    "def to_minimise(y):\n",
    "    qq_ij = low_dimensional_affinity(y)\n",
    "    return kl_div(p_ij_symmetric, qq_ij).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We return the results of the minimisation to the `res` object. \n",
    "Similar to the PCA results above, this can be plotted with a histogram to see how the t-SNE algorithm has transformed the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = minimize(to_minimise, pca.values[:, :2].flatten())\n",
    "tsne_transformed = res.x.reshape(-1, 2)\n",
    "data['tSNE1'] = tsne_transformed[:, 0]\n",
    "data['tSNE2'] = tsne_transformed[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(data=data, x='tSNE1', y='tSNE2', hue='label', kind='scatter')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear, that for this non-linearly discriminated data, the t-SNE algorithm is much better at finding a reduced dimensionality that allows the differences to be investigated. \n",
    "Let us continue to look at t-SNE, as it is available to us from `scikit-learn`. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "special-topics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
