{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis\n",
    "\n",
    "The first of the dimensionality reduction algorithms that we will look at is principal component analysis (PCA). \n",
    "PCA benefits from being computationally efficient when compared to some other dimensionality reduction approaches. \n",
    "Additionally it is possible to interpret the results -- tending towards ideas of explainable machine learning that we will meet again later. \n",
    "However, PCA assumes that there are linear trends in the data that can be used to bring different features together. \n",
    "This means that it may be less effective where the trends at non-linear. \n",
    "\n",
    "\n",
    "## What Is the Aim of PCA?\n",
    "\n",
    "The *aim* of PCA is to transform the data into a new coordinate system that is defined by the **principal components** of the data. \n",
    "These new axes (the principal components) are ordered by how much of the variance that is present in the original data they explain. \n",
    "A matrix with these principal components as columns, scaled by the amount of variance that each describes, produce a vector space where all of the variables are distributed with a standard deviation of 1 and a covariance of zero.\n",
    "Let's see that in action for a two-dimensional example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('./../data/pca-example.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is data with two columns, *x* and *y*, and 200 datapoints. \n",
    "We can visualise this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "data.plot(kind='scatter', x='x', y='y', ax=ax)\n",
    "ax.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we pass this data, as is, to a PCA algorithm (we will use the `scikit-learn`), we should get a pair of principal component vectors. \n",
    "\n",
    "````{margin}\n",
    "```{note}\n",
    "Take a look at the documentation for [`sklearn.decomposition.PCA`](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html), you will see that if the `n_components` is not given, then the number of features present will be used.\n",
    "```\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(data)\n",
    "pca.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the columns in this vector is a principal axis. \n",
    "We then scale then by their *explained variance* (or rather the squared root of the explained variance). \n",
    "````{margin}\n",
    "```{note}\n",
    "To ensure that the arrays are broadcast correctly, i.e., the explained variances weight the columns of the `components_`, we use `np.newaxis` to create a column of variances.\n",
    "```\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "weighted_components = pca.components_ / np.sqrt(pca.explained_variance_[:, np.newaxis])\n",
    "weighted_components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrix above can then be used to perform a linear transformation on the original data. \n",
    "We will add the results of the linear transformation into the original `data` object, as `x_prime` and `y_prime`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed = weighted_components @ data.T\n",
    "data['x_prime'] = transformed.loc[0]\n",
    "data['y_prime'] = transformed.loc[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the result of the transformation has a covariance matrix, that approximates to an identify matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[['x_prime', 'y_prime']].cov()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, when we plot this, the data are distributed as we would expect. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(8, 4))\n",
    "\n",
    "data.plot(kind='scatter', x='x', y='y', ax=ax[0])\n",
    "data.plot(kind='scatter', x='x_prime', y='y_prime', ax=ax[1])\n",
    "ax[0].axis('equal')\n",
    "ax[0].set_aspect('equal')\n",
    "ax[1].axis('equal')\n",
    "ax[1].set_aspect('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Are the Principal Components?\n",
    "\n",
    "The principal components are vectors that describe the (new) dimension of highest variance in the data. \n",
    "Each of the principal components must be orthogonal (i.e., at a right angle) to each other. \n",
    "So if we visualise the first and second principal components of the above data, we see that the first component will follow the direction of the data along the positive diagonal in the plot. \n",
    "Meanwhile, the second principal component must be orthogonal to this, so will sit at a right angle to the first. \n",
    "\n",
    "````{margin}\n",
    "```{note}\n",
    "These vectors have not been scaled, for example by the explained variance (the importance) of that principal component.\n",
    "```\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "for i, ax in enumerate(axes):\n",
    "    data.plot(kind='scatter', x='x', y='y', ax=ax)\n",
    "    ax.plot([-pca.components_[i, 0] * 3, pca.components_[i, 0] * 3], \n",
    "            [-pca.components_[i, 1] * 3, pca.components_[i, 1] * 3], color='k', lw=2)\n",
    "    ax.axis('equal')\n",
    "    ax.set_aspect('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where Does PCA Go Wrong?\n",
    "\n",
    "PCA assumes there is are linear relationships present in the data. \n",
    "Consider the following, moon-shaped, data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./../data/moon.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "data.plot(kind='scatter', x='x', y='y', ax=ax)\n",
    "ax.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear that there is some relationship present in the data that we should be aiming to describe. \n",
    "However, is we attempt to use the data's principal components to describe it, we find that both components are essentially equally weighted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pca.fit(data)\n",
    "pca.explained_variance_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This isn't surprising, if we visualise the principal components plotted on top of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "for i, ax in enumerate(axes):\n",
    "    data.plot(kind='scatter', x='x', y='y', ax=ax)\n",
    "    ax.plot([-pca.components_[i, 0] * 3, pca.components_[i, 0] * 3], \n",
    "            [-pca.components_[i, 1] * 3, pca.components_[i, 1] * 3], color='k', lw=2)\n",
    "    ax.axis('equal')\n",
    "    ax.set_aspect('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The components are not able to describe the curve present in the data, instead just sitting along $x=0$ and $y=0$.\n",
    "This is because the principal components can only be linear, and having non-linear relationships present in the data cannot be accurately modelled. \n",
    "Therefore, it is generally a good idea to check the linearity of your data before PCA is applied. \n",
    "```{warning}\n",
    "There is no hard rule for how linear data must be for using PCA successfully. \n",
    "However, it is important to think about the model (for PCA it is a linear one) that is being assumed. \n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "special-topics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
