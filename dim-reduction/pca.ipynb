{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis\n",
    "\n",
    "The first of the dimensionality reduction algorithms that we will look at is principal component analysis (PCA). \n",
    "PCA benefits from being computationally efficient compared to other dimensionality reduction approaches. \n",
    "Additionally, it is possible to interpret the results by tending towards ideas of explainable machine learning that we will meet again later. \n",
    "However, PCA assumes that there are linear trends in the data that can be used to bring different features together. \n",
    "This means that it may be less effective where the trends are non-linear. \n",
    "\n",
    "\n",
    "## What Is the Aim of PCA?\n",
    "\n",
    "The *aim* of PCA is to transform the data into a new coordinate system defined by the data's **principal components**. \n",
    "These new axes (the principal components) are ordered by how much of the variance is present in the original data they explain. \n",
    "A matrix with these principal components as columns, scaled by the amount of variance each describes, produces a vector space where all variables are distributed with a standard deviation of 1 and a zero covariance.\n",
    "Let's see that in action for a two-dimensional example.\n",
    "````{margin}\n",
    "```{note}\n",
    "The data file can be obtained [here](../data/pca-example.csv). \n",
    "```\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('./../data/pca-example.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data has two columns, *x* and *y*, and 200 datapoints. \n",
    "We can visualise this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "data.plot(kind='scatter', x='x', y='y', ax=ax)\n",
    "ax.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we pass this data, as is, to a PCA algorithm (we will use the `scikit-learn`), we should get a pair of principal component vectors. \n",
    "\n",
    "````{margin}\n",
    "```{note}\n",
    "Take a look at the documentation for [`sklearn.decomposition.PCA`](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html), you will see that if the `n_components` is not given, then the number of features present will be used.\n",
    "```\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(data)\n",
    "pca.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the columns in this vector is a principal axis. \n",
    "We then scale by their *explained variance* (or rather the squared root of the explained variance). \n",
    "````{margin}\n",
    "```{note}\n",
    "To ensure that the arrays are broadcast correctly, i.e., the explained variances weight the columns of the `components_`, we use `np.newaxis` to create a column of variances.\n",
    "```\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "weighted_components = pca.components_ / np.sqrt(pca.explained_variance_[:, np.newaxis])\n",
    "weighted_components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrix above can then be used to perform a linear transformation on the original data. \n",
    "We will add the results of the linear transformation into the original `data` object as `x_prime` and `y_prime`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed = weighted_components @ data.T\n",
    "data['x_prime'] = transformed.loc[0]\n",
    "data['y_prime'] = transformed.loc[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the result of the transformation has a covariance matrix, approximating an identify matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[['x_prime', 'y_prime']].cov()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, when we plot this, the data are distributed as expected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(8, 4))\n",
    "\n",
    "data.plot(kind='scatter', x='x', y='y', ax=ax[0])\n",
    "data.plot(kind='scatter', x='x_prime', y='y_prime', ax=ax[1])\n",
    "ax[0].axis('equal')\n",
    "ax[0].set_aspect('equal')\n",
    "ax[1].axis('equal')\n",
    "ax[1].set_aspect('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Are the Principal Components?\n",
    "\n",
    "The principal components are vectors that describe the (new) dimension of the highest variance in the data. \n",
    "Each principal component must be orthogonal (i.e., at a right angle) to each other. \n",
    "So, suppose we visualise the above data's first and second principal components. \n",
    "In that case, we see that the first component will follow the direction of the data along the positive diagonal in the plot. \n",
    "Meanwhile, the second principal component must be orthogonal to this so that it will sit at a right angle to the first. \n",
    "\n",
    "````{margin}\n",
    "```{note}\n",
    "These vectors have not been scaled, for example, by the explained variance (the importance) of that principal component.\n",
    "```\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "for i, ax in enumerate(axes):\n",
    "    data.plot(kind='scatter', x='x', y='y', ax=ax)\n",
    "    ax.plot([-pca.components_[i, 0] * 3, pca.components_[i, 0] * 3], \n",
    "            [-pca.components_[i, 1] * 3, pca.components_[i, 1] * 3], color='k', lw=2)\n",
    "    ax.axis('equal')\n",
    "    ax.set_aspect('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where Does PCA Go Wrong?\n",
    "\n",
    "PCA assumes there are linear relationships present in the data. \n",
    "Consider the following moon-shaped data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./../data/moon.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "data.plot(kind='scatter', x='x', y='y', ax=ax)\n",
    "ax.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some relationships in the data that we should aim to describe. \n",
    "However, is we attempt to use the data's principal components to describe it, we find that both are equally weighted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pca.fit(data)\n",
    "pca.explained_variance_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This isn't surprising if we visualise the principal components plotted on top of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "for i, ax in enumerate(axes):\n",
    "    data.plot(kind='scatter', x='x', y='y', ax=ax)\n",
    "    ax.plot([-pca.components_[i, 0] * 3, pca.components_[i, 0] * 3], \n",
    "            [-pca.components_[i, 1] * 3, pca.components_[i, 1] * 3], color='k', lw=2)\n",
    "    ax.axis('equal')\n",
    "    ax.set_aspect('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The components cannot describe the curve in the data; instead, they sit along $x=0$ and $y=0$.\n",
    "This is because the principal components can only be linear, and the non-linear relationships present in the data cannot be accurately modelled. \n",
    "Therefore, it is generally a good idea to check the linearity of your data before PCA is applied. \n",
    "```{warning}\n",
    "There is no hard rule for how linear data must be to use PCA successfully. \n",
    "However, it is essential to consider the model (for PCA, it is a linear one) that is being assumed. \n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "special-topics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
