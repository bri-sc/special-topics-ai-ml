{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using t-SNE in Practice\n",
    "\n",
    "Previously, we looked at the details of how the t-SNE algorithm works. \n",
    "However, as with many things in data science, the t-SNE algorithm has already been implemented in existing software. \n",
    "Specifcally, we are going to look at using t-SNE from `scikit-learn`. \n",
    "We will look at how t-SNE handles the MNIST handwritten digits data, so lets start by reading that file in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('./../data/mnist.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of working with the entire dataset, we will take just a subsample of the data, and work with that. \n",
    "This will help our t-SNE algorithm run a bit faster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsample = data.sample(n=1000, random_state=42)\n",
    "subsample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters of t-SNE\n",
    "\n",
    "Unlike PCA, t-SNE has some, so called, hyperparameters that can be used to control the implementation of the algorithm. \n",
    "We have already meet one of these, but in this section, we will highlight a few others. \n",
    "\n",
    "### Perplexity\n",
    "\n",
    "We will start by looking at the effect of changing the perplexity on the results from t-SNE. \n",
    "It was mentioned when discussing the algorithm, the perplexity balances between the local and global structure. \n",
    "Let's start with the default value for the `TSNE` object from `scikit-learn`, which is 30. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def control_hyperparameters(perplexity=30, learning_rate='auto', early_exaggeration=12):\n",
    "    dataset = subsample.copy()\n",
    "    tsne = TSNE(n_components=2, \n",
    "                perplexity=perplexity, \n",
    "                learning_rate=learning_rate, \n",
    "                early_exaggeration=early_exaggeration, \n",
    "                random_state=42)\n",
    "    tsne_result = tsne.fit_transform(dataset.drop('label', axis=1))\n",
    "    dataset['tSNE1'] = tsne_result[:, 0]\n",
    "    dataset['tSNE2'] = tsne_result[:, 1]\n",
    "    return dataset\n",
    "\n",
    "perplexity_30 = control_hyperparameters(perplexity=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above function can be reused to test different values. \n",
    "Let's look at how the data are distributed with a perplexity of 30. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 7))\n",
    "sns.scatterplot(x='tSNE1', y='tSNE2', hue='label', legend='full', data=perplexity_30, ax=ax)\n",
    "ax.set_aspect('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some clear clusters of given numbers. \n",
    "A low perplexity value is associated with local structure over global structure. \n",
    "However, for extremely small values of perplexity, this can lead to the formation of lots of small clusters. \n",
    "Consider a perplexity of value of 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexity_1 = control_hyperparameters(perplexity=1)\n",
    "fig, ax = plt.subplots(figsize=(9, 7))\n",
    "sns.scatterplot(x='tSNE1', y='tSNE2', hue='label', legend='full', data=perplexity_1, ax=ax)\n",
    "ax.set_aspect('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Meanwhile, a large perplexity value prioritise the global structure over the local. \n",
    "Again for extreme values, this can have undesirable results -- producing data lacks structure.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexity_600 = control_hyperparameters(perplexity=600)\n",
    "fig, ax = plt.subplots(figsize=(9, 7))\n",
    "sns.scatterplot(x='tSNE1', y='tSNE2', hue='label', legend='full', data=perplexity_600, ax=ax)\n",
    "ax.set_aspect('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The perplexity is probably the most important hyperparameter in the use of t-SNE. \n",
    "Unfortunately, there is no simple rule to selecting the value of perplexity and generally the best advice is to test a few different values and decide which value leads to a transformed data that is most informative for the problem at hand. \n",
    "\n",
    "### Learning Rate\n",
    "\n",
    "The learning rate effects the algorithm that is used to minimise the KL divergence. \n",
    "High learning rates can lead to faster convergence but may overshoot the minimum or cause instability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexity_1 = control_hyperparameters(learning_rate=5000)\n",
    "fig, ax = plt.subplots(figsize=(9, 7))\n",
    "sns.scatterplot(x='tSNE1', y='tSNE2', hue='label', legend='full', data=perplexity_1, ax=ax)\n",
    "ax.set_aspect('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Low learning rate will lead to a very slow convergence. \n",
    "\n",
    "### Early Exaggeration\n",
    "\n",
    "Another hyperparameter that has an impact on the minimisation process it the early exaggeration. \n",
    "This involves exaggerating the affinities in high dimensional space during the initial stages of the algorithm. \n",
    "This can be important in creating more meaningful clusters. \n",
    "\n",
    "We can see the impact of no early exaggaration for the MNIST dataset below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexity_1 = control_hyperparameters(early_exaggeration=1)\n",
    "fig, ax = plt.subplots(figsize=(9, 7))\n",
    "sns.scatterplot(x='tSNE1', y='tSNE2', hue='label', legend='full', data=perplexity_1, ax=ax)\n",
    "ax.set_aspect('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The impact is hard to see, this is probably the least important of the three hyperparameters identified here. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "special-topics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
