{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretation of PCA\n",
    "\n",
    "One of the most significant benefits of PCA over other dimensionality reduction approaches is that the results of PCA can be interpreted. \n",
    "Here we will look at interpretating the principal components from some data, specifically the breast cancer data highlighted previously. \n",
    "We can read in the data, scale it and perform the PCA analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "data = pd.read_csv('./../data/breast-cancer.csv')\n",
    "scaled_data = StandardScaler().fit_transform(data[data.columns[1:]])\n",
    "pca = PCA()\n",
    "transformed = pca.fit_transform(scaled_data)\n",
    "pc = pd.DataFrame(transformed, columns=['PC{}'.format(i + 1) for i in range(transformed.shape[1])])\n",
    "pc['Diagnosis'] = data['Diagnosis']\n",
    "pc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Many Principal Components to Investigate?\n",
    "\n",
    "A common question with many dimensionality reduction algorithms centres on the idea of how many dimensions do we want to end up with?\n",
    "The answer to this question is usually very problem specific and depends on what information you want to gain about your data. \n",
    "However, for PCA, the scree plot is a useful tool to help us visualise the information present in each component. \n",
    "\n",
    "The scree or elbow plot involves plotting the explained variance (or explained variance ratio) as a function of components. \n",
    "So for the breast cancer dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(range(1, len(pca.explained_variance_ratio_) + 1), pca.explained_variance_ratio_, '.-')\n",
    "ax.set_xlabel('Principal Component')\n",
    "ax.set_ylabel('Proportion of Variance Explained')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of the scree or elbow plot is to identify the \"elbow\" in the dataset. \n",
    "This is a subjective approach, but the elbow is essentially the point at which adding more components doesn't not add a significant gain in explained variance. \n",
    "So for the plot above, I would say that the elbow is around three principal components, so the first two should be sufficient for most interpretations. \n",
    "\n",
    "A slightly older approach is to use principal components that have eigenvalues greater than one, recall that the eigenvalue of the covariance matrix is the explained variance. \n",
    "We can plot this so called Kaiser criterion (or KL1) on our scree plot to see if this agrees with our suggestion from the elbow method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(range(1, len(pca.explained_variance_) + 1), pca.explained_variance_, '.-')\n",
    "ax.axhline(1, color='#ff7f0e', linestyle='--', label='KL Criterion')\n",
    "ax.set_xlabel('Principal Component')\n",
    "ax.set_ylabel('Proportion of Variance Explained')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Do the Principal Components Mean?\n",
    "\n",
    "It is possible to use the principal components and their eigenvalues to derive an interpretation of the data. \n",
    "This can be extremely powerful in understanding the terns and relationship that lead to our observations. \n",
    "The first interpretative tool is the loading matrix, this gives a quantitative description of how each feature contributes to a given principal component. \n",
    "The loading of the *n*th principal component is the *n*-th row of the component matrix (where the principal components themselves are the columns) multiplied by the *n*th eigenvalue.\n",
    "The code below calculates the loading matrix for the first two principal components and plots them against the relevant features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "loading_matrix = pd.DataFrame(pca.components_[:2].T * np.sqrt(pca.explained_variance_[:2]), \n",
    "                              columns=['PC1', 'PC2'], index=data.columns[1:11])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "loading_matrix.plot(kind='bar', ax=ax)\n",
    "ax.set_xlabel('Features')\n",
    "ax.set_ylabel('Loading')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the above plot, that the first principal component contains significant positive loading from most of the features. \n",
    "This means that a positive value for the transformed data into the first principal component is associated with a positive value for all of the features. \n",
    "On the other hand, the second principal component has a negative loading for many of the features (radius, texture, etc.). \n",
    "This means that a positive value for the transformed data into the second principal component is associated with a negative value for these features. \n",
    "\n",
    "Let's look at the data transformed by the first and second principal components, we have coloured this data depending on whether the cancer was found to be malignant or benign. \n",
    "````{margin}\n",
    "```{note}\n",
    "Here, the `seaborn` library is used to create a nice statistical plot. \n",
    "Seaborn is a nice wrapper around the `matplotlib` library that makes creating common statistical plots easier. \n",
    "```\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.jointplot(x='PC1', y='PC2', hue='Diagnosis', data=pc, kind='scatter')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this it is clear that benign tumours typically had less of principal component 1. \n",
    "Looking at this in the context of the loading matrix plot above, we can rationalise that, typically, malignant tumours have larger values of the features, such as concave points, than benign tumours.\n",
    "One could imagine using a clustering algorithm on principal component 1, to classify new data is benign or malignant. \n",
    "Principal component 2 is harder to interpret, as the distinction is less clear.\n",
    "This is expected, given that principal component 2 explains only 25 % of the variance, compared against principal component 1's 60 %. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "special-topics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
