{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Regressors\n",
    "\n",
    "Support vector regressors are built on many of the concepts of support vector machines, but modified for use in regression problems instead of classification. \n",
    "This means using a slightly different loss function, so instead of the hinge loss function, the &epsilon;-insensitive loss function is used. \n",
    "\n",
    "In support vector regression instead of optimising a hyperplane, an &epsilon;-tube is created around the true regression line. \n",
    "The optimisation is then to find a (typically linear) function, $f(x)$, the deviates from the target values by no more than &epsilon; for as many data points as possible. \n",
    "All while keeping the model complexity to a minimum. \n",
    "\n",
    "Support vector regression can still make use of the kernel trick that makes support vector machines so powerful. \n",
    "Let's look at using support vector regressors for the student performance dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.read_csv('../data/student-performance.csv')  \n",
    "data['Encoded EA'] = [1 if x == 'Yes' else 0 for x in data['Extracurricular Activities']]\n",
    "train, test = train_test_split(data, test_size=0.2, random_state=42)\n",
    "X = train.drop(['Performance Index', 'Extracurricular Activities'], axis=1)\n",
    "y = train['Performance Index']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the `sklearn.svm.SVM` method, the `sklearn.svm.SVR` method defaults to apply the [RBF kernel trick](/classification/kernel-trick.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "svr = SVR()\n",
    "svr.fit(X, y)\n",
    "\n",
    "X_test = test.drop(['Performance Index', 'Extracurricular Activities'], axis=1)\n",
    "y_test = test['Performance Index']\n",
    "\n",
    "mean_squared_error(y_test, svr.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the RBF kernel produces a worse model than linear regression. \n",
    "We can make sense of this, as the data are clearly data (we saw this for the polynomial regression). \n",
    "Therefore, it would make sense to apply a differen kernel, i.e., the linear one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_lin = SVR(kernel='linear')\n",
    "svr_lin.fit(X, y)\n",
    "\n",
    "X_test = test.drop(['Performance Index', 'Extracurricular Activities'], axis=1)\n",
    "y_test = test['Performance Index']\n",
    "\n",
    "mean_squared_error(y_test, svr_lin.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This returns an MSE very similar to the previous methods."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "special-topics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
