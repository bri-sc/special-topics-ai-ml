{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polynomial Regression\n",
    "\n",
    "An extension of linear regression is polynomial regression. \n",
    "This is where the independent variables are modelled as *n*-degree polynomials. \n",
    "This captures non-linear behaviour, which is impossible for linear models. \n",
    "However, this comes at the risk of overfitting, where the polynomial *n* is too large. \n",
    "In polynomial regression, the matrix $\\mathbf{X}$ is expanded to include polynomial terms among the independent variables (the features of the data). \n",
    "This can include cross-terms between different parameters. \n",
    "\n",
    "## A Quadratic Example \n",
    "Let's consider an example where *n* is 2. \n",
    "Again, we will use the student performance dataset to see if the additional parameters can improve the mean-squared error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.read_csv('../data/student-performance.csv')  \n",
    "data['Encoded EA'] = [1 if x == 'Yes' else 0 for x in data['Extracurricular Activities']]\n",
    "train, test = train_test_split(data, test_size=0.2, random_state=42)\n",
    "X = train.drop(['Performance Index', 'Extracurricular Activities'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mathematics of polynomial regression is the same matrix equation discussed for linear regression; however, now there are many more parameters, $\\beta$, being estimated. \n",
    "The student performance data has five features that we are training against. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, if we add all the cross- and self-polynomial terms, there will be many more \"features\" to train against. \n",
    "If there are features $a$ and $b$ in linear regression, we would have features $a$, $b$, $a^2$, $ab$ and $b^2$, so you can see how the number of features can balloon. \n",
    "There is a `sklearn.preprocessing` method to perform this extension of the matrix $\\mathbf{X}$; we set `include_bias` to `False` as the scikit-learn implementation of `LinearRegression handles this for us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_ = poly.fit_transform(X)\n",
    "y = train['Performance Index']\n",
    "X_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This *new* feature set can be put into the standard `LinearRegression` method, as the matrix equations are the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "linear_regression = LinearRegression()\n",
    "linear_regression.fit(X_, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then check the model against the test dataset and compute the mean-squared error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X_test = test.drop(['Performance Index', 'Extracurricular Activities'], axis=1).values\n",
    "X_test_ = poly.fit_transform(X_test)\n",
    "y_test = test['Performance Index'].values\n",
    "mean_squared_error(y_test , linear_regression.predict(X_test_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is an improvement in the MSE; however, it is not substantial. \n",
    "This indicates that the linear model was probably sufficient to describe the trends. \n",
    "We can see this by investigating the coefficients $\\beta$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_regression.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should observe that the zeroth to fourth values in the `coef_` array are more significant than the other terms. \n",
    "These larger values are associated with the linear terms, while the smaller ones are the polynomial terms, which we accept are not important to model the data. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "special-topics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
