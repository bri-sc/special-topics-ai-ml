{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel Tricks\n",
    "\n",
    "So far, our support vector machines are only realistically able to classify data where there is a linear trend. \n",
    "While, this is often the case -- in particular in high dimensional spaces of neural networks --  is it valuable to use these tools on non-linear systems. \n",
    "The approach in SVMs is to harness the *kernel trick*, akin to something we have seen in linear algebra. \n",
    "\n",
    "This trick, is a transformation of the space that the data are in. \n",
    "Usually, this transformation involves taking data to a high-dimensional space, that creates a linear discrimation in the data. \n",
    "A popular example of a kernel trick is for one-dimentional data, where each data point is an integer. \n",
    "Consider the classification of odd and even numbers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "odd = np.arange(1, 11, 2)\n",
    "even = np.arange(2, 11, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is impossible in the current mapping. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(odd, np.zeros_like(odd), 'o')\n",
    "ax.plot(even, np.zeros_like(even), 'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, by computing the modulus of `x`, we can see a linear discrimination between the classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(odd, np.mod(odd, 2), 'o')\n",
    "ax.plot(even, np.mod(even, 2), 'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that a support vector machine hyperplane above would sit along $y = 0.5$. \n",
    "\n",
    "````{margin}\n",
    "```{note}\n",
    "Hello, [Carl Friedrich Gauss](https://en.wikipedia.org/wiki/Carl_Friedrich_Gauss)!\n",
    "```\n",
    "````\n",
    "Naturally, this is an simplistic approach. \n",
    "One of the most popualr kernels, the default for the `scikit-learn` implementation of support vector machines is the radial basis function kernel. \n",
    "This kernel is based on a Gaussian distribution and remaps data to a higher dimensionality space. \n",
    "In many ways, this is can be thought of as the inverse of the dimensionality reduction approach of [t-SNE](../dim-reduction/tsne.html)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "special-topics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
