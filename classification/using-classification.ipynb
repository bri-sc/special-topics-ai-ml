{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Classification Methods\n",
    "\n",
    "Let's now compare the different classification methods we have discussed. \n",
    "Instead of using our own implementations, we will harness the more efficient implementations of `scikit-learn`. \n",
    "We will test each of them with the [breast cancer dataset](../setup/datasets), so we can start by loading this dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('./../data/breast-cancer.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a labelled dataset where the labels are either Malignant or Benign. \n",
    "To use these in many of our algorithms, we need to encode these to numerical values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Encoded Diagnosis'] = data['Diagnosis'].apply(lambda x: 1 if x == 'Malignant' else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split the data into our usual training and test subsets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will then scale the data. \n",
    "This is not necessarily required for all of the algorithms, but for consistency, we will use it in all cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_train = scaler.fit_transform(train.drop(['Diagnosis', 'Encoded Diagnosis'], axis=1))\n",
    "scaled_test = scaler.fit_transform(test.drop(['Diagnosis', 'Encoded Diagnosis'], axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since all methods have a shared API, we can use a loop to perform each method in turn. \n",
    "For each method, we train using the training share and then make predictions on the test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "methods = {'Logistic Regression': LogisticRegression(random_state=42),\n",
    "           'SVM': SVC(random_state=42),\n",
    "           'Random Forest': RandomForestClassifier(random_state=42)}\n",
    "\n",
    "for k, v in methods.items():\n",
    "    v.fit(scaled_train, train['Encoded Diagnosis'])\n",
    "    test[f'{k} Prediction'] = v.predict(scaled_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "\n",
    "To quantify the success of a machine learning workflow, numerical quality scores are necessary. \n",
    "So far, we have used `accuracy_score`; however, this is not an ideal metric as it only accounts for when the algorithm has identified *true positives*. \n",
    "Other popular metrics include precision, recall, and the combinations of these two, the F<sub>1-score. \n",
    "Precision tells us how the many of the samples that are identified as malignant were, in fact, malignant, \n",
    "\n",
    "$$\n",
    "\\text{precision} = \\frac{N(\\text{true positives})}{N(\\text{true positives})+ N(\\text{false positives})}.\n",
    "$$\n",
    "\n",
    "The recall answers how many of the malignant samples were correctly identified by the algorithm, \n",
    "\n",
    "$$\n",
    "\\text{recall} = \\frac{N(\\text{true positives})}{N(\\text{true positives})+ N(\\text{false negatives})}.\n",
    "$$\n",
    "\n",
    "Finally, the F<sub>1</sub>-score balances these two and is a valuable tool when a single metric is needed. \n",
    "\n",
    "$$\n",
    "F_1 = \\frac{2\\times\\text{precision}\\times\\text{recall}}{\\text{precision}+\\text{recall}}.\n",
    "$$\n",
    "\n",
    "The true and false positives and negatives are described in {numref}`metrics`. \n",
    "\n",
    "```{figure} ../images/metrics.png\n",
    "---\n",
    "name: metrics\n",
    "width: 60%\n",
    "---\n",
    "A figure showing the identification of true and false positives and negatives that make up the precision and recall scores. \n",
    "```\n",
    "\n",
    "These metrics are computed with `sklearn`, as shown below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "for k in methods.keys():\n",
    "    print(f'{k} Precision: {precision_score(test[\"Encoded Diagnosis\"], test[f\"{k} Prediction\"]):.3f}')\n",
    "    print(f'{k} Recall: {recall_score(test[\"Encoded Diagnosis\"], test[f\"{k} Prediction\"]):.3f}')\n",
    "    print(f'{k} F1-Score: {f1_score(test[\"Encoded Diagnosis\"], test[f\"{k} Prediction\"]):.3f}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All three methods do very well in the classification. \n",
    "Indeed, the scikit-learn implementation outperform the implementations we wrote ourselves. \n",
    "However, from comparing the F<sub>1</sub>-scores, the support vector machine is the most effective. \n",
    "We highlight here that the implementations used are na√Øve, in that there is no hyperparameter optimisation being used. \n",
    "To achieve this, one could consider performing some random search or optimisation over the hyperparameter space. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "special-topics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
