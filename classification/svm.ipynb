{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines\n",
    "\n",
    "````{margin}\n",
    "```{note}\n",
    "You may note that support vector machines also suffer from problems regarding non-linearity. \n",
    "But this approach takes advantage of linear algebra to reconfigure the space if necessary. \n",
    "```\n",
    "````\n",
    "An alternative classification to logistic regression is support vector machines. \n",
    "The aim of the support vector machine (SVM) is to find the best linear *hyperplane* that separates the classes in feature space. \n",
    "This is achieved by maximising the, so-called, margin, which is the distance between the hyperplane and the nearest data points from each class. \n",
    "These data points are the support vectors. \n",
    "\n",
    "```{figure} ../images/svm.png\n",
    "---\n",
    "name: svm\n",
    "width: 40%\n",
    "---\n",
    "The margin (represented with the red shaded area) describes the region that separates the yellow and blue support vectors (thick black edge). \n",
    "```\n",
    "\n",
    "Similar to logistic regression, the hyperplane that an SVM tries to find has the equation, \n",
    "\n",
    "$$\n",
    "\\mathbf{w}\\cdot\\mathbf{x} + b = 0.\n",
    "$$\n",
    "\n",
    "The two hyperplanes that describe the edges of the margin can be written as\n",
    "\n",
    "$$\n",
    "\\mathbf{w}\\cdot\\mathbf{x} + b = 1,\n",
    "$$\n",
    "\n",
    "where the items are of class 1, and \n",
    "\n",
    "$$\n",
    "\\mathbf{w}\\cdot\\mathbf{x} + b = -1,\n",
    "$$\n",
    "\n",
    "for class -1. \n",
    "````{margin}\n",
    "```{note}\n",
    "See the following [math.stackexchange discussion](https://math.stackexchange.com/questions/1305925/why-is-the-svm-margin-equal-to-frac2-mathbfw) to see why the distance is $\\frac{2}{|\\mathbf{w}|}$. \n",
    "```\n",
    "````\n",
    "The distance between these two hyperplanes is $\\frac{2}{|w|}$, so by minimising $|\\mathbf{w}|$, the maximise this distance.\\\n",
    "However, is it also necessary to keep data points on their correct side of the margin, which is achieved with an appropriate constraint. \n",
    "\n",
    "## Soft Margin\n",
    "\n",
    "It may be impossible to find a hyperplane that completely separates the classes. \n",
    "For example, consider the data from the [toy example for logistic regression](./logistic-regression.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "data = pd.read_csv(\"../data/toy.csv\")\n",
    "\n",
    "fix, ax = plt.subplots()\n",
    "sns.scatterplot(x='x', y='y', data=data, hue='label', ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It it clear that due to the overlap, there is no hyperplane possible. \n",
    "SVMs handle this through the inclusion in of what is known as a *soft margin*. \n",
    "The soft margin introduces some allowance for misclassification of data points. \n",
    "This leads the loss function to be the **hinge loss**, \n",
    "\n",
    "$$\n",
    "L = \\frac{1}{2}|\\mathbf{w}|^2 + C\\sum_{i=1}^{n}\\textrm{max}(0, 1 - y_i(\\mathbf{w}\\cdot \\mathbf{x}_i + b)),\n",
    "$$\n",
    "\n",
    "where $C$ is the hyperparameter that controls the trade off between maximising the margin and minimising misclassification.\n",
    "\n",
    "## Putting It Into Practice\n",
    "\n",
    "Let's see if we can develop a support vector machine to classify our toy data. \n",
    "Once again, we will split the data, we delibrately split it the same way as previously, to enable comparison, and create separate objects for the training dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data['encoded_label'] = [1 if i == 'b' else 0 for i in data['label'].values]\n",
    "train, test = train_test_split(data, test_size=0.2, random_state=0)\n",
    "X = train[['x', 'y']].values\n",
    "y = train['encoded_label'].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we generate our initial weights and bias. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "rng = np.random.default_rng(1)\n",
    "weights = rng.standard_normal(2)\n",
    "bias = rng.standard_normal(1)\n",
    "weights, bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to define the loss function, learning rate and $C$. \n",
    "Again, we use JAX here, to help our gradient descent algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import grad\n",
    "\n",
    "C = 0.1\n",
    "learning_rate = 0.001\n",
    "\n",
    "def loss_function(weights, bias):\n",
    "    prediction = y * (jnp.dot(X, weights) + bias)\n",
    "    stack = jnp.hstack([jnp.zeros_like(y), 1 - prediction])\n",
    "    soft_margin = C * jnp.mean(jnp.max(stack, axis=0))\n",
    "    return 0.5 * jnp.linalg.norm(weights) ** 2 + soft_margin\n",
    "\n",
    "first_order = grad(loss_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now run 2000 iterations of the gradient descent algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = grad(loss_function)(weights, bias)\n",
    "\n",
    "for i in range(2000):\n",
    "    weights = weights - learning_rate * result[:-1]\n",
    "    bias = bias - learning_rate * result[-1]\n",
    "    result = grad(loss_function)(weights, bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we compute the accuracy score against the training and test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "z = np.dot(train[['x', 'y']], weights) + bias\n",
    "f = np.sign(z)\n",
    "prediction = np.array(['nd'] * len(z))\n",
    "prediction[np.where(f > 0)] = 'b'\n",
    "prediction[np.where(f < 0)] = 'a'\n",
    "train['prediction'] = prediction\n",
    "z\n",
    "accuracy_score(train['label'], train['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.dot(test[['x', 'y']], weights) + bias\n",
    "f = np.sign(z)\n",
    "prediction = np.array(['x'] * len(f))\n",
    "prediction[np.where(f > 0)] = 'b'\n",
    "prediction[np.where(f < 0)] = 'a'\n",
    "test['prediction'] = prediction\n",
    "\n",
    "accuracy_score(test['label'], test['prediction'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives similar, but slightly different, values to the logistic regression.\n",
    "This is to be expected, as we are optimising different functions in order to enable the discrimiation. \n",
    "\n",
    "Thus far, both of these methods are limited to data with a linear discrimination. \n",
    "Let's look now at a tool employed by support vector machines to help in this area. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "special-topics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
