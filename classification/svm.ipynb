{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines\n",
    "\n",
    "````{margin}\n",
    "```{note}\n",
    "You may note that support vector machines also suffer from problems regarding non-linearity. \n",
    "However, this approach uses linear algebra to reconfigure the space if necessary. \n",
    "```\n",
    "````\n",
    "An alternative classification to logistic regression is support vector machines. \n",
    "The support vector machine (SVM) aims to find the best linear *hyperplane* that separates the classes in feature space. \n",
    "This is achieved by maximising the so-called margin, which is the distance between the hyperplane and the nearest data points from each class. \n",
    "These data points are the support vectors. \n",
    "\n",
    "```{figure} ../images/svm.png\n",
    "---\n",
    "name: svm\n",
    "width: 40%\n",
    "---\n",
    "The margin (represented with the red shaded area) describes the region that separates the yellow and blue support vectors (thick black edge). \n",
    "```\n",
    "\n",
    "Similar to logistic regression, the hyperplane that an SVM tries to find has the equation, \n",
    "\n",
    "$$\n",
    "\\mathbf{w}\\cdot\\mathbf{x} + b = 0.\n",
    "$$\n",
    "\n",
    "The two hyperplanes that describe the edges of the margin can be written as\n",
    "\n",
    "$$\n",
    "\\mathbf{w}\\cdot\\mathbf{x} + b = 1,\n",
    "$$\n",
    "\n",
    "where the items are of class 1, and \n",
    "\n",
    "$$\n",
    "\\mathbf{w}\\cdot\\mathbf{x} + b = -1,\n",
    "$$\n",
    "\n",
    "for class -1. \n",
    "````{margin}\n",
    "```{note}\n",
    "See the following [math.stackexchange discussion](https://math.stackexchange.com/questions/1305925/why-is-the-svm-margin-equal-to-frac2-mathbfw) to see why the distance is $\\frac{2}{|\\mathbf{w}|}$. \n",
    "```\n",
    "````\n",
    "The distance between these two hyperplanes is $\\frac{2}{|w|}$, so by minimising $|\\mathbf{w}|$, the maximise this distance.\\\n",
    "However, keeping data points on their correct side of the margin is also necessary, which is achieved with an appropriate constraint. \n",
    "\n",
    "## Soft Margin\n",
    "\n",
    "It may be impossible to find a hyperplane that completely separates the classes. \n",
    "For example, consider the data from the [toy example for logistic regression](./logistic-regression.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "data = pd.read_csv(\"../data/toy.csv\")\n",
    "\n",
    "fix, ax = plt.subplots()\n",
    "sns.scatterplot(x='x', y='y', data=data, hue='label', ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear that due to the overlap, no hyperplane is possible. \n",
    "SVMs handle this by including what is known as a *soft margin*. \n",
    "The soft margin introduces some allowance for misclassification of data points. \n",
    "This leads the loss function to be the **hinge loss**, \n",
    "\n",
    "$$\n",
    "L = \\frac{1}{2}|\\mathbf{w}|^2 + C\\sum_{i=1}^{n}\\textrm{max}(0, 1 - y_i(\\mathbf{w}\\cdot \\mathbf{x}_i + b)),\n",
    "$$\n",
    "\n",
    "where $C$ is the hyperparameter that controls the trade-off between maximising the margin and minimising misclassification.\n",
    "\n",
    "## Putting It Into Practice\n",
    "\n",
    "Let's see if we can develop a support vector machine to classify our toy data. \n",
    "Once again, we will split the data in the same way as previously to enable comparison and create separate objects for the training dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data['encoded_label'] = [1 if i == 'b' else 0 for i in data['label'].values]\n",
    "train, test = train_test_split(data, test_size=0.2, random_state=0)\n",
    "X = train[['x', 'y']].values\n",
    "y = train['encoded_label'].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we generate our initial weights and bias. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "rng = np.random.default_rng(1)\n",
    "weights = rng.standard_normal(2)\n",
    "bias = rng.standard_normal(1)\n",
    "weights, bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to define the loss function, learning rate and $C$. \n",
    "Again, we use JAX here to help our gradient descent algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import grad\n",
    "\n",
    "C = 0.1\n",
    "learning_rate = 0.001\n",
    "\n",
    "def loss_function(weights, bias):\n",
    "    prediction = y * (jnp.dot(X, weights) + bias)\n",
    "    stack = jnp.hstack([jnp.zeros_like(y), 1 - prediction])\n",
    "    soft_margin = C * jnp.mean(jnp.max(stack, axis=0))\n",
    "    return 0.5 * jnp.linalg.norm(weights) ** 2 + soft_margin\n",
    "\n",
    "first_order = grad(loss_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now run 2000 iterations of the gradient descent algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = grad(loss_function)(weights, bias)\n",
    "\n",
    "for i in range(2000):\n",
    "    weights = weights - learning_rate * result[:-1]\n",
    "    bias = bias - learning_rate * result[-1]\n",
    "    result = grad(loss_function)(weights, bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the accuracy score against the training and test data as before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "z = np.dot(train[['x', 'y']], weights) + bias\n",
    "f = np.sign(z)\n",
    "prediction = np.array(['nd'] * len(z))\n",
    "prediction[np.where(f > 0)] = 'b'\n",
    "prediction[np.where(f < 0)] = 'a'\n",
    "train['prediction'] = prediction\n",
    "z\n",
    "accuracy_score(train['label'], train['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.dot(test[['x', 'y']], weights) + bias\n",
    "f = np.sign(z)\n",
    "prediction = np.array(['x'] * len(f))\n",
    "prediction[np.where(f > 0)] = 'b'\n",
    "prediction[np.where(f < 0)] = 'a'\n",
    "test['prediction'] = prediction\n",
    "\n",
    "accuracy_score(test['label'], test['prediction'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives similar, but slightly different, values to the logistic regression.\n",
    "This is expected, as we are optimising different functions to enable the discrimination. \n",
    "\n",
    "Thus far, both methods are limited to data with linear discrimination. \n",
    "Let's look at a tool employed by support vector machines to help in this area. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "special-topics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
