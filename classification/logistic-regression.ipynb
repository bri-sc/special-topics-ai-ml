{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "Logistic regression is confusingly named, because it is used for binary classification, not regression. \n",
    "It is called regression as a regression model is used to describe the relationship between the input features and the probability of a specific outcome. \n",
    "To get started, let's have a look at the logistic function, which sits at the heart of the logistic regression algorithm. \n",
    "\n",
    "## Logistic Function\n",
    "\n",
    "````{margin}\n",
    "```{note}\n",
    "Another sigmoid function that you may have seen is the cumulative distribution function of a normal distribution. \n",
    "This function is known as the [error function](https://en.wikipedia.org/wiki/Error_function). \n",
    "```\n",
    "````\n",
    "\n",
    "The logistic function is a type of sigmoid function that can be used in any area of mathematics, \n",
    "\n",
    "$$\n",
    "f(z) = \\frac{1}{1 + \\exp{(-z)}}, \n",
    "$$\n",
    "\n",
    "where $z$ is the input variable. \n",
    "Let's plot the logistic function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def logistic(z):\n",
    "    \"\"\"\n",
    "    Compute the logistic function\n",
    "    :param z: input\n",
    "    \n",
    "    :return: output of the logistic function\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "z = np.linspace(-10, 10, 100)\n",
    "logistic_z = logistic(z)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(z, logistic_z)\n",
    "ax.set_xlabel('z')\n",
    "ax.set_ylabel('logistic(z)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear that the logistic function can only have values of between 0 and 1. \n",
    "This enables the binary classification, where one class is associated with the value 0 and the other associated with the value 1. \n",
    "Therefore, if the value of *z* (we will come to how *z* is calculated in a bit) is 7.5, the logistic function is has a value of: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic(7.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, this would be of class 1. \n",
    "However, what if the value of *z* is 0.1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we round, using rounding half up rules. \n",
    "So this would also be of class 1.\n",
    "\n",
    "The logistic function (and other sigmoidal functions) is extremely important in binary classification approaches. \n",
    "These can be generalised to the classification of $N$ classes using softmax regression (also known as multinomial logistic regression). \n",
    "\n",
    "### Softmax Regression\n",
    "\n",
    "Softmax regression uses a multinomial (multidimensional) logistic probability distribution with the following form, \n",
    "\n",
    "$$\n",
    "P(j|\\mathbf{z}) = \\frac{\\exp{(z_i)}}{\\sum_{j=1}^{K}\\exp{(z_j)}}\n",
    "$$\n",
    "\n",
    "where $\\mathbf{z}$ is a vector of length $K$ and each member of the vector is associated with a class $j$. \n",
    "So if there are three classes; a, b, and c, for which the vector $\\mathbf{z}$ is: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.array([2.0, 1.0, 0.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute the softmax function using `scipy`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "\n",
    "softmax(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the vector $\\mathbf{z}$ has the following probabilities for each class, given the vector $\\mathbf{z}$: \n",
    "- $P(a|\\mathbf{z}) = 62.9\\%$\n",
    "- $P(b|\\mathbf{z}) = 23.1\\%$\n",
    "- $P(c|\\mathbf{z}) = 14.0\\%$\n",
    "\n",
    "So, we would say that the vector $\\mathbf{z}$ is most likely to describe something in class a. \n",
    "The use of a logistic function, means there can be a probability associated with the classification, which is not possible for all classification algorithms. \n",
    "\n",
    "## Linear Combination\n",
    "\n",
    "The input to the logistic function is found from a linear combination of our input features, $\\mathbf{x}$.\n",
    "Each of these input features is weighted, we can think of these weights $\\mathbf{w}$, as how important they are in the classification and finally, a bias term, $b$, is added, giving,\n",
    "\n",
    "$$\n",
    "z = \\sum_{i=1}^{N}{(w_ix_i)} + b,\n",
    "$$\n",
    "\n",
    "where $N$ is the features in the data.\n",
    "We can also write this function as a dot product, \n",
    "\n",
    "$$\n",
    "z = \\mathbf{w} \\cdot \\mathbf{x} + b.\n",
    "$$\n",
    "\n",
    "It is these weights and bias term that we optimise in the training part of our machine learning workflow. \n",
    "\n",
    "## Optimisation\n",
    "\n",
    "````{margin}\n",
    "```{note}\n",
    "You should remind yourself of the step size parameter in the gradient descent algorithm.\n",
    "```\n",
    "````\n",
    "Typically, [gradient descent](/maths/optimisation.html#gradient-descent-method) or similar algorithms are used to optimise the values of the weights and bias. \n",
    "This leads to another common term from the machine learning world, *learning rate*. \n",
    "Since we are using gradient descent, it is necessary that a step size for the optimisation is defined. \n",
    "This is the learning rate. \n",
    "\n",
    "The final thing to consider is *what* we are actually optimising. \n",
    "This is called the *loss function* of the algorithm, a common loss function for logistic regression is the cross entropy loss, \n",
    "\n",
    "$$\n",
    "L(\\mathbf{z}) = -\\frac{1}{m}\\sum_{i=1}^{m}\\left\\{y_i \\log{\\left[f(z_i)\\right]} + (1 - y_i) \\log{\\left[1 - f(z_i)\\right]}\\right\\},\n",
    "$$\n",
    "\n",
    "where the summation if over the $m$ training data points, each with the value $y_i$, and $f(z_i)$ is the model result using the logistic function discussed above. \n",
    "Let's look at applying logistic regression to some example datasets. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "special-topics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
