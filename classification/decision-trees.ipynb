{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees \n",
    "\n",
    "Decision trees are a popular approach to classification. \n",
    "A decision tree is made up of a series of nodes that represent \"choices\" based on some feature of the data. \n",
    "The branches then represent the outcome of this. \n",
    "By following a path through the decision tree, we can predict the class of a given data point. \n",
    "\n",
    "```{figure} ../images/decision-trees.png\n",
    "---\n",
    "name: decision-trees\n",
    "width: 80%\n",
    "---\n",
    "Three decision trees, where each node is a choice and edge is an outcome. The terminal node are known as leaves.\n",
    "```\n",
    "\n",
    "One of the advantages of decision trees, is that for many systems the decision making process can be understood and interpretable. \n",
    "We should be conscious of the risk of overfitting with decision trees that grow very deep, but there are approaches that can be used to address this. \n",
    "\n",
    "## Composition of a Decision Tree\n",
    "\n",
    "The decision tree starts with a root node.\n",
    "This root node splits the data based on the most significant feature present. \n",
    "Following this, there are a number of internal node that test individual features of the data. \n",
    "Finally, the leaf nodes represent the final class prediction and after these there are no further splits. \n",
    "\n",
    "The natural question is, how are the data split in each node. \n",
    "The aim of the splitings is to either maximise the information gain from the split, or minimise the *impurity*. \n",
    "This is achieved with a splitting criteria, such as the Gini impurity: a measure of the likelihood that the classification is incorrect.\n",
    "\n",
    "## Building a Decision Tree\n",
    "\n",
    "Continuing the practice from other classification approaches, we can create a decision tree algorithm from scratch for the toy data that we have been investigating. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.read_csv(\"../data/toy.csv\")\n",
    "\n",
    "data['encoded_label'] = [1 if i == 'b' else 0 for i in data['label'].values]\n",
    "train, test = train_test_split(data, test_size=0.2, random_state=0)\n",
    "X = train[['x', 'y']].values\n",
    "y = train['encoded_label'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to create a function that computes the Gini impurity. \n",
    "The Gini impurity is computed for a given dataset, $D$ as,\n",
    "\n",
    "$$\n",
    "G(D) = 1 = \\sum_{i+1}^{C} p_i^2,\n",
    "$$\n",
    "\n",
    "where $C$ is the number of classes and $p_i$ is the proportion of elements that below to the class $i$. \n",
    "Therefore, this can be computed with the function below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def gini_impurity(labels):\n",
    "    \"\"\"\n",
    "    Calculate the Gini impurity of a list of labels.\n",
    "    \n",
    "    :param y: a list of labels\n",
    "    \n",
    "    :return: the Gini impurity of the list\n",
    "    \"\"\"\n",
    "    _, counts = np.unique(labels, return_counts=True)\n",
    "    p = (counts / labels.size) ** 2\n",
    "    return 1 - np.sum(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If all of the labels belong to the same class, we have a pure node with a Gini impurity of 0. \n",
    "Let's compute the Gini impurity for the current training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gini_impurity(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About 0.5, indicating an even split between the two classes (the data is maximally impure). \n",
    "This is to be expected as the full dataset is 50 % class `a` and 50 % class `b`. \n",
    "\n",
    "The next step is in a given iteration of the training, to find the best way to split the data. \n",
    "This is achieve by finding all of the potential *thresholds* for splitting each feature of the data. \n",
    "The data is then split across each of these thresholds in turn, and compute the information that is gained (using the Gini impurity) from such as splitting. \n",
    "By iterating through all of the thresholds, we can find the best splitting which maximises the information gain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_split(X, y):\n",
    "    \"\"\"\n",
    "    Determines the best split for a dataset based\n",
    "    on the Gini impurity.\n",
    "    \n",
    "    :param X: a 2D numpy array of features\n",
    "    :param y: a 1D numpy array of labels\n",
    "    \n",
    "    :return: a tuple of the best feature, best threshold, and best gain\n",
    "    \"\"\"\n",
    "    best_gain = 0\n",
    "    best_feature = None\n",
    "    best_threshold = None\n",
    "\n",
    "    thresholds = np.unique(X, axis=0)\n",
    "    for feature in range(X.shape[1]):\n",
    "        for threshold in thresholds:\n",
    "            y_left = y[np.where(X[:, feature] < threshold[feature])]\n",
    "            y_right = y[np.where(X[:, feature] >= threshold[feature])]\n",
    "\n",
    "            weight_left = y_left.size / y.size\n",
    "            weight_right = y_right.size / y.size\n",
    "            information_gain = gini_impurity(y) - (weight_left * gini_impurity(y_left) + weight_right * gini_impurity(y_right))\n",
    "\n",
    "            if information_gain > best_gain:\n",
    "                best_gain = information_gain\n",
    "                best_feature = feature\n",
    "                best_threshold = threshold[feature]\n",
    "\n",
    "    return best_feature, best_threshold, best_gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will return the index of the best feature to split, the threshold value and how much information will be gained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_best_split(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````{margin}\n",
    "```{note}\n",
    "The decision tree provides a really nice example of [recursion](https://en.wikipedia.org/wiki/Recursion_(computer_science)), which is an important concept in computer science. \n",
    "```\n",
    "````\n",
    "Then to actually use the algorithm, we recursively run through the function below until there is either no information to be gained or there is no way left to split the data (i.e., we have found all the leaves). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tree(X, y):\n",
    "    \"\"\"\n",
    "    Recursively constructs a decision tree.\n",
    "    \n",
    "    :param X: a 2D numpy array of features\n",
    "    :param y: a 1D numpy array of labels\n",
    "    \n",
    "    :returns: a dictionary representing the decision tree\n",
    "    \"\"\"\n",
    "    if len(np.unique(y)) == 1:\n",
    "        return np.argmax(np.bincount(y))\n",
    "    \n",
    "    feature, threshold, gain = find_best_split(X, y)\n",
    "\n",
    "    if gain == 0:\n",
    "        return np.argmax(np.bincount(y))\n",
    "\n",
    "    left_X = X[np.where(X[:, feature] < threshold)]\n",
    "    left_y = y[np.where(X[:, feature] < threshold)]\n",
    "    right_X = X[np.where(X[:, feature] >= threshold)]\n",
    "    right_y = y[np.where(X[:, feature] >= threshold)]\n",
    "\n",
    "    left_subtree = make_tree(left_X, left_y)\n",
    "    right_subtree = make_tree(right_X, right_y)\n",
    "\n",
    "    return {\"feature\": feature,\n",
    "            \"threshold\": threshold, \n",
    "            \"gain\": gain, \n",
    "            \"left\": left_subtree, \n",
    "            \"right\": right_subtree}\n",
    "\n",
    "tree = make_tree(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this implementation, the `tree` object is a dictionary, that we can visualise with the `plot_tree` helper function, which can be downloaded [here](./helper.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import plot_tree\n",
    "\n",
    "plot_tree(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The visualisation isn't perfect, but you can get an idea of the structure of the tree. \n",
    "\n",
    "With the tree constructed, to predict from it, it is simply a matter of traversing the tree for a given sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traverse_tree(x, tree):\n",
    "    \"\"\"\n",
    "    Traverse a decision tree to make a prediction.\n",
    "    \n",
    "    :param x: a 1D numpy array of features\n",
    "    :param tree: a dictionary representing the decision tree\n",
    "    \n",
    "    :return: the prediction\n",
    "    \"\"\"\n",
    "    if not isinstance(tree, dict):\n",
    "        return tree\n",
    "\n",
    "    feature = tree[\"feature\"]\n",
    "    threshold = tree[\"threshold\"]\n",
    "\n",
    "    if x[feature] <= threshold:\n",
    "        return traverse_tree(x, tree[\"left\"])\n",
    "    else:\n",
    "        return traverse_tree(x, tree[\"right\"])\n",
    "    \n",
    "traverse_tree(train.iloc[0].values, tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the first value in the training dataset is predicted to be of class `a`. \n",
    "\n",
    "Let's repeat this for all the data points in the training and test datasets to compute our accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "prediction = np.array([traverse_tree(x, tree) for x in train[['x', 'y']].values])\n",
    "prediction_label = np.array(['nd'] * len(prediction))\n",
    "prediction_label[np.where(prediction == 1)] = 'b'\n",
    "prediction_label[np.where(prediction == 0)] = 'a'\n",
    "train['prediction'] = prediction_label\n",
    "\n",
    "accuracy_score(train['label'], train['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = np.array([traverse_tree(x, tree) for x in test[['x', 'y']].values])\n",
    "prediction_label = np.array(['nd'] * len(prediction))\n",
    "prediction_label[np.where(prediction == 1)] = 'b'\n",
    "prediction_label[np.where(prediction == 0)] = 'a'\n",
    "test['prediction'] = prediction_label\n",
    "\n",
    "accuracy_score(test['label'], test['prediction'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this toy dataset, the decision tree was capable of getting a very high accuracy. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "special-topics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
