{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimisation\n",
    "\n",
    "Optimisation is an important concept when we consider the application of computation and mathematics to the physical sciences. \n",
    "A common goal in chemistry and physics is to find the configuration of atoms that minimises the energy of some system. \n",
    "This is typically achieved, computationally, through some optimisation algorithm. \n",
    "This algorithm will iteratively propose new configurations of atoms until that with the minimum energy is found. \n",
    "\n",
    "```{admonition} Minimisation vs Maximisation\n",
    ":class: tip\n",
    "Optimisation algorithms may be used to minimise some function (in the example above, the energy of the system is the function) or maximise a function (for example, it can be desirable to maximise some probability distribution). \n",
    "For all intents and purposes, when it comes to optimisation, these are equivalent. \n",
    "You can convince yourself of this by thinking of a maximisation of some function as the minimisation of its negative. \n",
    "Indeed, as we will see, this is practically how this is often achieved. \n",
    "```\n",
    "\n",
    "Naturally, the simplest approach to find the minimum of some function would be:\n",
    "1. Select a range of values to compute the function for. \n",
    "2. Evenly distribute $n$ points on that that grid. \n",
    "3. Compute the function for these points. \n",
    "4. Find the point where the function is smallest. \n",
    "However, there are clear issues with this algorithm, in particular that the increasing the number of dimensions (variables) in your problem, increasing the computation exponentially. \n",
    "Instead, let's look at some algorithms that are a bit smarter about the approach they take. \n",
    "\n",
    "## Gradient Descent Method\n",
    "\n",
    "````{margin}\n",
    "```{note}\n",
    "You can also think of this as a *gradient ascent* algorithm, by flipping the appropriate signs.\n",
    "```\n",
    "````\n",
    "The gradient descent method, also known as steepest descent, can be thought of as following the path of a ball down a hill. \n",
    "The ball will find the bottom of the hill (the minimum of our function) if we let it roll for long enough (perform enough iterations of the algorithm).\n",
    "This is achived by computing the gradient of the function we want to minimise, using differentiation. \n",
    "Let's look now at the algorithm, consider trying to find the value of $x$ that optimises the function $f(x)$: \n",
    "\n",
    "1. Starting with some initial guess for $x$.\n",
    "2. Calculate the gradient $\\frac{\\text{d}f(x)}{x}$, at $x$. \n",
    "3. Change the value of $x$ so that the value of $f(x)$ decreases, i.e., change $x$ by some amount $\\Delta x$ in the direction opposite to the gradient. \n",
    "4. Repeat 2 and 3 until the gradient if close to zero. \n",
    "\n",
    "````{margin}\n",
    "```{note}\n",
    "The Lennard-Jones potential is a common functional model to describe the energy of two noble gas atoms.\n",
    "```\n",
    "````\n",
    "As an example, let's consider the minimisation (optimisation) of a classical function from computational chemistry/physics, known as the Lennard-Jones potential. \n",
    "This function has the following form, \n",
    "\n",
    "$$\n",
    "E(r) = \\frac{A}{r^{12}} - \\frac{B}{r^{6}}\n",
    "$$\n",
    "\n",
    "where, $A$ and $B$ are system specific coefficients and $r$ is the distance between two atoms. \n",
    "Below, we plot a Lennard-Jones potential that is commonly used to model argon atoms {cite}`Rahman1964`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = 1.58e-134 \n",
    "B = 1.02e-77\n",
    "sigma = 3.4 #angstrom\n",
    "\n",
    "from scipy.constants import k\n",
    "\n",
    "epsilon = 0.0103 #eV\n",
    "\n",
    "A = 4 * epsilon * sigma ** 12\n",
    "B = 4 * epsilon * sigma ** 6\n",
    "A, B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def lennard_jones(r, A, B):\n",
    "    \"\"\"\n",
    "    Computes the first derivative of the Lennard-Jones \n",
    "    potential.\n",
    "\n",
    "    :param r: Distance between atoms \n",
    "    :param A: First coefficient.\n",
    "    :param B: Second coefficient.\n",
    "\n",
    "    :returns: First derivative of Lennard-Jones at r.\n",
    "    \"\"\"\n",
    "    return A / r ** 12 - B / r ** 6\n",
    "\n",
    "A = 98320.5\n",
    "B = 63.6\n",
    "r = np.linspace(3.2, 7.2, 1000)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(r, lennard_jones(r, A, B))\n",
    "ax.set_xlabel('$r$ / Å')\n",
    "ax.set_ylabel('$E(r)$ / eV')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````{margin}\n",
    "```{note}\n",
    "The underscores are so that the `sympy.symbols` have different names to the variables created above.\n",
    "```\n",
    "````\n",
    "We can then use `sympy` to find the derivative. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy import symbols, diff\n",
    "\n",
    "r_, A_, B_ = symbols('r A B')\n",
    "lj = A_ / r_ ** 12 - B_ / r_ ** 6\n",
    "diff(lj, r_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When can then use this to define a function that computes the derivative. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_derivative(r, A, B):\n",
    "    \"\"\"\n",
    "    Computes the first derivative of the Lennard-Jones \n",
    "    potential.\n",
    "\n",
    "    :param r: Distance between atoms \n",
    "    :param A: First coefficient.\n",
    "    :param B: Second coefficient.\n",
    "\n",
    "    :returns: First derivative of Lennard-Jones at r.\n",
    "    \"\"\"\n",
    "    return -12 * A / r ** 13 + 6 * B / r ** 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then put a simple gradient descent algorithm together as follows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iterations = 1000\n",
    "initial_r = 5\n",
    "delta_r = 0.02\n",
    "\n",
    "r_values = np.array([initial_r])\n",
    "for i in range(n_iterations):\n",
    "    first = first_derivative(r_values[-1], A, B)\n",
    "    if np.isclose(np.abs(first), 0):\n",
    "        break\n",
    "    r_values = np.append(r_values, r_values[-1] - np.sign(first) * delta_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, the algorithm can run for 1000 iterations, but if the first derivative is very small, close to zero, we stop early. \n",
    "We can see how many iterations have run by looking at the length of the `r_values` array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(r_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it took all 1001 steps, indicating that is may not have found the minimum. \n",
    "Let's have a look at the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_values[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the last ten values iterated through were just moving back and forth between 3.80 and 3.82 Å. \n",
    "This is because the minimum is somewhere between these two values and the step size is 0.02 Å.\n",
    "We can visualise this path on the function.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(r, lennard_jones(r, A, B))\n",
    "ax.plot(r_values[:-1], lennard_jones(r_values[:-1], A, B), 'o', alpha=0.5, label='Iterations')\n",
    "ax.plot(r_values[-1], lennard_jones(r_values[-1], A, B), 'ko', label='Final Result')\n",
    "ax.set_xlabel('$r$ / Å')\n",
    "ax.set_ylabel('$E(r)$ / eV')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient descent algorithm, clearly has a very important hyperparameter, the step size $\\Delta x$. \n",
    "This hyperparameter defines *how long* the algorithm will take to reach the minimum.\n",
    "There is a small improvement that is commonly used, where the gradient is used to scale the step size, i.e., instead of just using the gradient's sign.\n",
    "This will mean that when the gradient is close to 0, the step size will be smaller. \n",
    "````{margin}\n",
    "```{warning}\n",
    "Note the difference between this algorithm and the one above.\n",
    "```\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iterations = 1000\n",
    "initial_r = 5\n",
    "delta_r = 1.5\n",
    "\n",
    "r_values = np.array([initial_r])\n",
    "for i in range(n_iterations):\n",
    "    first = first_derivative(r_values[-1], A, B)\n",
    "    if np.isclose(np.abs(first), 0):\n",
    "        break\n",
    "    r_values = np.append(r_values, r_values[-1] - first * delta_r)\n",
    "len(r_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, a solution was found in just 298 steps. \n",
    "So the improvement worked! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(r, lennard_jones(r, A, B))\n",
    "ax.plot(r_values[:-1], lennard_jones(r_values[:-1], A, B), 'o', alpha=0.5, label='Iterations')\n",
    "ax.plot(r_values[-1], lennard_jones(r_values[-1], A, B), 'ko', label='Final Result')\n",
    "ax.set_xlabel('$r$ / Å')\n",
    "ax.set_ylabel('$E(r)$ / eV')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear that with this scaled step size, we can find the minimum more easily. \n",
    "However, there is still the presence of a hyperparameter, can we adapt this algorithm to remove the hyperparameter?\n",
    "\n",
    "## The Newton-Raphson Method\n",
    "\n",
    "````{margin}\n",
    "```{note}\n",
    "After (co-)inventing calculus, Isaac Newton went on to put it to use in optimsation, or root finding. \n",
    "In this example, we use Newton's notative for derivatives, $f'(x)$ being the first derivative. \n",
    "Previously, we used Leibniz's notation, $\\frac{\\text{d}f(x)}{\\text{d}x}$. \n",
    "```\n",
    "````\n",
    "An adaptation which does not require any hyperparameters is known as a the Newton-Raphson method. \n",
    "The adaption involves changing the function that updates $x$, which is now, \n",
    "\n",
    "$$\n",
    "x_{\\text{new}} = x_{\\text{old}} - \\frac{f'(x_{\\text{old}})}{f''{x_{\\text{old}}}}, \n",
    "$$\n",
    "\n",
    "where $f'(x_{\\text{old}})$ and $f''(x_{\\text{old}})$ are the first and second derivatives at $x_{\\text{old}}$ of the function being optimised. \n",
    "To update the algorithm above with this new update function, we need the second derivative of the Lennard-Jones function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff(lj, r_, r_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def second_derivative(r, A, B):\n",
    "    \"\"\"\n",
    "    Computes the second derivative of the Lennard-Jones \n",
    "    potential.\n",
    "\n",
    "    :param r: Distance between atoms \n",
    "    :param A: First coefficient.\n",
    "    :param B: Second coefficient.\n",
    "\n",
    "    :returns: First derivative of Lennard-Jones at r.\n",
    "    \"\"\"\n",
    "    return 6 * (26 * A / r ** 6 - 7 * B) / r ** 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the second derivative, we can now implement the algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iterations = 1000\n",
    "initial_r = 4\n",
    "\n",
    "r_values = np.array([initial_r])\n",
    "for i in range(n_iterations):\n",
    "    first = first_derivative(r_values[-1], A, B)\n",
    "    second = second_derivative(r_values[-1], A, B)\n",
    "    if np.abs(first) < 5e-12:\n",
    "        break\n",
    "    r_values = np.append(r_values, r_values[-1] - first / second)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(r, lennard_jones(r, A, B))\n",
    "ax.plot(r_values[:-1], lennard_jones(r_values[:-1], A, B), 'o', alpha=0.5, label='Iterations')\n",
    "ax.plot(r_values[-1], lennard_jones(r_values[-1], A, B), 'ko', label='Final Result')\n",
    "ax.set_xlabel('$r$ / Å')\n",
    "ax.set_ylabel('$E(r)$ / eV')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(r_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time the algorithm only took 7 steps to reach the minimum, but you may notice that the initial guess was 4 Å instead of the 5 Å that was used previously. \n",
    "This is due to a drawback of the Newton-Raphson method, that we can understand by plotting the second derivative. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "r_tight = np.linspace(4, 5, 100)\n",
    "\n",
    "ax.plot(r_tight, second_derivative(r_tight, A, B))\n",
    "ax.axhline(0, color='k', ls='--')\n",
    "ax.set_xlabel('$r$ / Å')\n",
    "ax.set_ylabel(\"$E''(r)$ / eV Å$^{-2}$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At a distance between particles of around 4.2 Å, the second derivative goes from positive to negative. \n",
    "This means that the first derivative has moved through a stationary point at this value. \n",
    "Furthermore, the Newton-Raphson method will try to move the particles further apart, rather than closer together. \n",
    "Let's see that in action. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iterations = 1000\n",
    "initial_r = 5\n",
    "\n",
    "r_values = np.array([initial_r])\n",
    "for i in range(n_iterations):\n",
    "    first = first_derivative(r_values[-1], A, B)\n",
    "    second = second_derivative(r_values[-1], A, B)\n",
    "    if np.isclose(np.abs(first), 0):\n",
    "        break\n",
    "    r_values = np.append(r_values, r_values[-1] - first / second)\n",
    "\n",
    "r_wide = np.linspace(r.min(), r_values.max(), 1000)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(r_wide, lennard_jones(r_wide, A, B))\n",
    "ax.plot(r_values[:-1], lennard_jones(r_values[:-1], A, B), 'o', alpha=0.5, label='Iterations')\n",
    "ax.plot(r_values[-1], lennard_jones(r_values[-1], A, B), 'ko', label='Final Result')\n",
    "ax.set_xlabel('$r$ / Å')\n",
    "ax.set_ylabel('$E(r)$ / eV')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This indicates the importance of a good starting point in running some optimisation algorithm. \n",
    "\n",
    "These two algorithms are just the start of a journey that has become a whole field of mathematics. \n",
    "We won't look in detail at more complex gradient descent style algorithms, but many build on the principles outlined here. \n",
    "\n",
    "## `scipy.optimize.minimize`\n",
    "\n",
    "The `scipy` package implements a [vast range](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html) of minimisation algorithms. \n",
    "These cover a wide variety of problems, such as the presence of constraints or bounds in the variables. \n",
    "The `minimize` function is able to dynamically select an appropriate algorithm for the problem at hand. \n",
    "Let's look at how we can use it.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "res = minimize(lennard_jones, x0=[5], args=(A, B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the `minimize` function takes a few arguments, the first is the function to minimise, the second `x0` is the initial guess of the parameters and `args` is the non-variable parameters that should be passed to the function being minimised. \n",
    "There are a range of other keyword arguments that can be found in the documentation. \n",
    "The `minimize` function returns a special type called an `OptimizeResult`, let's have a look at the contents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important information are:\n",
    "- `success`: This says if the optimisation was successful. \n",
    "- `x`: The optimised value of the parameter(s).\n",
    "- `nit`: The number of iterations. \n",
    "\n",
    "We can see that the algorithm selected by `scipy.optimize.minimize` was able to find the solution, in just four iterations. \n",
    "\n",
    "So far we have focused on optimisation problems where there is only one local minimum, which is also the global minimum. \n",
    "However, many problems in science have many local minima that we want to avoid to find the global solution. \n",
    "We will look later at some global optimisation approaches. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "special-topics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
