{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eigen*things*\n",
    "\n",
    "If we consider the non-zero vector, $\\mathbf{v} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$, shown in {numref}`eigen-trans`.\n",
    "There is a matrix $\\mathbf{A}$ that describes a linear transformation of $\\mathbf{v}$ to $\\mathbf{v'}$, which scales the vector by a scaler, $\\lambda$ (in {numref}`eigen-trans` $\\lambda=3$).\n",
    "We can write this as a matrix-vector multiplication $\\mathbf{A}\\mathbf{v} = \\mathbf{v'}$, which can be generalised as:\n",
    "\n",
    "$$\n",
    "\\mathbf{A}\\mathbf{v} = \\lambda\\mathbf{v}.\n",
    "$$ (eigenthings) \n",
    "\n",
    "````{margin}\n",
    "```{note}\n",
    "The prefix *eigen*- is adopted from the German word eigen, meaning 'proper', 'characteristic', or 'own'.\n",
    "```\n",
    "````\n",
    "\n",
    "$\\mathbf{v}$ is an eigenvector of the linear transformation $\\mathbf{A}$, while $\\lambda$ is the corresponding eigenvalue. \n",
    "\n",
    "```{figure} ../images/eigen-trans.png\n",
    "---\n",
    "name: eigen-trans\n",
    "height: 215px\n",
    "---\n",
    "A linear transformation is described by the matrix $\\mathbf{A}$, which is an eigenfunction of the vector $\\mathbf{v}$. \n",
    "```\n",
    "\n",
    "Eigenvalues and eigenvectors have broad applicability across science and engineering; therefore, we will see them come up a few times in this course book. \n",
    "The application that is most familiar to those with a physics or chemistry background is probably in the Schrödinger equation. \n",
    "Though, eigenvalues and eigenvectors also play an important role in data analysis approaches, such as principal components analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the Eigenvalues and Eigenvectors of a Matrix\n",
    "\n",
    "For many applications, the matrix is known and we want to find the relevant eigenvalues and eigenvectors. \n",
    "Consider, for example, the time-independent Schrödinger equation, which is used to calculate the energy of the wavefunction some quantum mechanical systems, has the form,\n",
    "\n",
    "$$ \n",
    "H \\psi_E = E \\psi_E,\n",
    "$$\n",
    "\n",
    "where, $H$ is the Hamiltonian matrix, $\\psi_E$ is the wavefunction and $E$ is the energy. \n",
    "It is clear that this has the same form, as Eqn. {eq}`eigenthings`, where $\\psi_E$ is the eigenvector and $E$ is the eigenvalue. \n",
    "Therefore, being able to calculate the eigenvalues and eigenvectors of the Hamiltonian matrix would give us the energy and wavefunction of our system. \n",
    "\n",
    "For a square-matrix the solution to find the eigenvalues and eigenvectors can be considered as a rearrangement of Eqn. {eq}`eigenthings`, followed by finding the roots of a quadratic equation.\n",
    "Firstly, both sides of Eqn. {eq}`eigenthings` are multiplied by an identity matrix, $\\mathbf{I}$ of the same shape as $\\mathbf{A}$, \n",
    "\n",
    "$$\n",
    "\\mathbf{A}\\mathbf{I}\\mathbf{v} = \\lambda\\mathbf{I}\\mathbf{v}.\n",
    "$$\n",
    "\n",
    "Any matrix multiplied by an identity matrix, is just the original matrix, i.e., $\\mathbf{A}\\mathbf{I} = \\mathbf{A}$. \n",
    "Therefore, we can write\n",
    "\n",
    "$$\n",
    "\\mathbf{A}\\mathbf{v} = \\lambda\\mathbf{I}\\mathbf{v}, \n",
    "$$\n",
    "\n",
    "rearrange, \n",
    "\n",
    "$$\n",
    "\\mathbf{A}\\mathbf{v} - \\lambda\\mathbf{I}\\mathbf{v} = 0,\n",
    "$$\n",
    "\n",
    "and take $\\mathbf{v}$ out as a common factor, \n",
    "\n",
    "$$\n",
    "(\\mathbf{A} - \\lambda\\mathbf{I})\\mathbf{v} = 0.\n",
    "$$\n",
    "\n",
    "Then, for any non-zero vector $\\mathbf{v}$, \n",
    "\n",
    "$$\n",
    "|\\mathbf{A} - \\lambda\\mathbf{I}| = 0,\n",
    "$$ (eigen-solve)\n",
    "\n",
    "which is to say that the determinant of $\\mathbf{A} - \\lambda\\mathbf{I}$ is equal to 0. \n",
    "To show this second step, it is easiest to use an example. \n",
    "Consider the matrix $\\mathbf{A}$ that describes the linear transformation shown in {numref}`eigen-trans`,\n",
    "\n",
    "$$\n",
    "\\mathbf{A} = \\begin{bmatrix} 2 & 1 \\\\ 1 & 2 \\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "We write this in the form of Eqn. {eq}`eigen-solve`, \n",
    "\n",
    "$$\n",
    "\\text{det} \\left(\\begin{bmatrix} 2 & 1 \\\\ 1 & 2 \\end{bmatrix} - \\begin{bmatrix} \\lambda & 0 \\\\ 0 & \\lambda \\end{bmatrix}\\right) = 0.\n",
    "$$\n",
    "\n",
    "Therefore, \n",
    "\n",
    "$$\n",
    "\\text{det} \\left(\\begin{bmatrix} 2 - \\lambda & 1 \\\\ 1 & 2 - \\lambda \\end{bmatrix}\\right) = 0, \n",
    "$$\n",
    "\n",
    "and calculating the determinant, using $ad - bc$, \n",
    "\n",
    "$$\n",
    "(2 - \\lambda)(2 - \\lambda) - 1\\times 1 = 3 - 4\\lambda + \\lambda^2 = (\\lambda - 3)(\\lambda - 1) = 0,\n",
    "$$\n",
    "\n",
    "returns the eigenvalues of $\\lambda = 3$ and $\\lambda = 1$. \n",
    "Then to find the eigenvector for $\\lambda = 3$, we find the solution to $(\\mathbf{A} - 3\\mathbf{I})\\mathbf{v} = 0$,\n",
    "\n",
    "$$\n",
    "\\left(\\begin{bmatrix} 2 & 1 \\\\ 1 & 2 \\end{bmatrix} - \\begin{bmatrix} 3 & 0 \\\\ 0 & 3 \\end{bmatrix}\\right) \\begin{bmatrix} v_1 \\\\ v_2 \\end{bmatrix} = \\begin{bmatrix} -1 & 1 \\\\ 1 & -1 \\end{bmatrix} \\begin{bmatrix} v_1 \\\\ v_2 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "We can expand these matrix multiplications for clarity, so, \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "-v_1 + v_2 & = 0 \\\\\n",
    "v_1 - v_2 & = 0.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "These two conditions can hold true for any vector where $v_1$ and $v_2$ are equal, i.e., $\\mathbf{v}$ in {numref}`eigen-trans`. \n",
    "The second eigenvector for the matrix $\\mathbf{A}$ is found by substituting $\\lambda = 1$. \n",
    "\n",
    "$$\n",
    "\\left(\\begin{bmatrix} 2 & 1 \\\\ 1 & 2 \\end{bmatrix} - \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}\\right) \\begin{bmatrix} v_1 \\\\ v_2 \\end{bmatrix} = \\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix} \\begin{bmatrix} v_1 \\\\ v_2 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "Again, for clarity, we expand the matrix multiplication,\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "v_1 + v_2 & = 0 \\\\\n",
    "v_1 + v_2 & = 0,\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "which can only hold true for vectors where $v_1 = -v_2$. \n",
    "\n",
    "### Eigenvalues and Eigenvectors with NumPy\n",
    "\n",
    "Naturally, NumPy can be used to find the eigenvalues and eigenvectors of a given matrix. \n",
    "We achieve this with the [`np.linalg.eig`](https://numpy.org/doc/2.1/reference/generated/numpy.linalg.eig.html) function.\n",
    "Let's use this to check the results of the mathematics above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "A = np.array([[2, 1], [1, 2]])\n",
    "eigenthings = np.linalg.eig(A)\n",
    "print(eigenthings.eigenvalues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the same eigenvalues were determined. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(eigenthings.eigenvectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The eigenvectors are a little more complex to parse. \n",
    "If we look at the documentation for the `np.linalg.eig` function, it can help us to understand, specifically the following text:\n",
    "> eigenvectors: The normalized (unit \"length\") eigenvectors, such that the column ``eigenvectors[:,i]`` is the eigenvector corresponding to the eigenvalue ``eigenvalues[i]``.\n",
    "\n",
    "So the columns of the `eigenthings.eigenvectors` are the normalised eigenvectors for each eigenvalue. \n",
    "This also agrees with what we have shown above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eigendecomposition\n",
    "\n",
    "Any square matrix can be broken down into a set of simpler components. \n",
    "This is known as eigendecomposition and, as we shall see, has significant utility in science and engineering.\n",
    "The eigendecomposition of the matrix $\\mathbf{A}$ is expressed as:\n",
    "\n",
    "$$\n",
    "\\mathbf{A} = \\mathbf{Q}\\mathbf{\\Lambda}\\mathbf{Q}^{-1},\n",
    "$$ (eigendecomposition)\n",
    "\n",
    "where $\\mathbf{Q}$ is a matrix whose columns are the eigenvectors of $\\mathbf{A}$ and $\\mathbf{\\Lambda}$ is a diagonal matrix whose diagonal entries are the eigenvalues of $\\mathbf{A}$. \n",
    "Therefore, using the eigendecomposition logic, we should be able to reconstruct $\\mathbf{A}$ from these constituent parts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = eigenthings.eigenvectors\n",
    "Lambda = np.diag(eigenthings.eigenvalues)\n",
    "Q @ Lambda @ np.linalg.inv(Q)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intensive",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
