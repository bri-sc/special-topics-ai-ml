{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Your Own GMM\n",
    "\n",
    "To look at the Gaussian mixture models algorithm, let's use the same data that [caused problems for *k*-means previously](../data/skew.csv). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "\n",
    "data = pd.read_csv('../data/skew.csv')\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "sns.scatterplot(x='x1', y='x2', data=data, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is common practice to start the GMM algorithm from an estimate of the cluster centres, for example, from *k*-means clustering.\n",
    "We assign these centres to the Gaussian mixture model's means, $\\mu_k$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "means = KMeans(n_clusters=3).fit(data).cluster_centers_\n",
    "means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the three mean values are two-dimensional; luckily, there is a `scipy.stats` object that describes an *N*-dimensional Gaussian distribution. \n",
    "This is called `scipy.stats.multivariate_normal`; let's look at the documentation for this object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "multivariate_normal?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that it takes means and a covariance matrix, the latter is, for a two-dimensional distribution, a 2&times;2 matrix. \n",
    "A good starting point for this matrix would be the covariance matrix of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covs = np.array([np.cov(data.T) for i in range(3)])\n",
    "covs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create the three two-dimensional Gaussian distributions that we will use to describe the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "mvs = [multivariate_normal(mean=mean, cov=cov) for mean, cov in zip(means, covs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can plot them alongside the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = np.linspace(data['x1'].min(), data['x1'].max(), 1000)\n",
    "x2 = np.linspace(data['x2'].min(), data['x2'].max(), 1000)\n",
    "\n",
    "X1, X2 = np.meshgrid(x1, x2)\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for i, ax in enumerate(ax):\n",
    "    ax.contour(X1, X2, mvs[i].pdf(np.dstack((X1, X2))), cmap='Grays')\n",
    "    ax.scatter(data['x1'], data['x2'])\n",
    "    ax.set_title(f'Cluster {i}')\n",
    "    ax.set_xlabel('x1')\n",
    "    ax.set_ylabel('x2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not a very good estimate of the clusters. \n",
    "But, we can use the EM algorithm of Gaussian mixture models to improve the estimates. \n",
    "\n",
    "First, let's look at computing the *responsibilities*, the responsibility, $\\gamma_{i, k}$ of a given data point, $x_i$, for Gaussian distribution $k$, where $k \\in K$, is computed as, \n",
    "\n",
    "$$\n",
    "\\gamma_{i, k} = \\frac{\\pi_k \\mathcal{N}(x_i | \\mu_k, \\Sigma_k)}{\\sum_{j=1}^{K}\\left[\\pi_j \\mathcal{N}(x_i | \\mu_j, \\Sigma_j)\\right]},\n",
    "$$\n",
    "\n",
    "where $\\pi_k$ is the weight of cluster $k$ (all the weights should sum to 1), and we read $\\mathcal{N}(x_i | \\mu_k, \\Sigma_k)$ as the probability of $x_i$ **given** the normal distribution $\\mathcal{N}(\\mu_k, \\Sigma_k)$. \n",
    "This can be thought of as the probability of finding a given data point in a given normal distribution. \n",
    "We can write code to achieve this expectation step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expectation(data, mvs, weights):\n",
    "    \"\"\"\n",
    "    Computate the responsibility for the data points given the current cluster MVs. \n",
    "    \n",
    "    :param data: The data points.\n",
    "    :param mvs: The cluster MVs.\n",
    "    :param weights: The weights of the clusters.\n",
    "    \n",
    "    :return: The responsibilities for the data points.\n",
    "    \"\"\"\n",
    "    responsiblities = np.zeros((data.shape[0], len(mvs)))\n",
    "    for i, mv in enumerate(mvs):\n",
    "        responsiblities[:, i] = weights[i] * mv.pdf(data)\n",
    "    responsiblities /= responsiblities.sum(axis=1)[:, np.newaxis]\n",
    "    return responsiblities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the responsibilities calculated, it is time to update the parameters: the means, covariance matrices and weights.\n",
    "This is achieved in the maximisation step. \n",
    "\n",
    "The mean for cluster $k$ is computed as the sum of the product of the responsibilities, and the data is divided by the sum of the responsibilities. \n",
    "\n",
    "$$\n",
    "\\mu_k = \\frac{\\sum_{i=1}^{N}\\left(\\gamma_{i, k} x_i\\right)}{\\sum_{i=1}^{N}\\left(\\gamma_{i, k}\\right)}. \n",
    "$$\n",
    "\n",
    "````{margin}\n",
    "```{note}\n",
    "Be aware that this implementation of the `new_means` function is vectorised to perform it for all $K$ clusters in a single operation. \n",
    "```\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def means_update(responsiblities, data):\n",
    "    \"\"\"\n",
    "    Compute the new means given the responsibilities and the data points.\n",
    "    \n",
    "    :param responsiblities: The responsibilities for the data points.\n",
    "    :param data: The data points.\n",
    "    \n",
    "    :return: The new means.\n",
    "    \"\"\"\n",
    "    return np.dot(responsiblities.T, data) / responsiblities.sum(axis=0)[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new covariances are computed with the difference between the data and the updated mean positions, \n",
    "\n",
    "$$\n",
    "\\Sigma_k = \\frac{\\sum_{i=1}^{N}\\left\\{\\left[\\gamma_{i,k}(x_i - \\mu_k)\\right]^\\top(x_i - \\mu_k)\\right\\}}{\\sum_{i=1}^{N}\\left(\\gamma_{i, k}\\right)}.\n",
    "$$\n",
    "\n",
    "Again, we can write this in Python as,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def covariances_update(responsiblities, data, means):\n",
    "    \"\"\"\n",
    "    Compute the new covariances given the responsibilities, data points, and means.\n",
    "    \n",
    "    :param responsiblities: The responsibilities for the data points.\n",
    "    :param data: The data points.\n",
    "    :param means: The means of the clusters.\n",
    "    \n",
    "    :return: The new covariances.\n",
    "    \"\"\"\n",
    "    covs = np.zeros((len(means), data.shape[1], data.shape[1]))\n",
    "    for i in range(responsiblities.shape[1]):\n",
    "        diff = data - means[i]\n",
    "        covs[i] = np.dot((responsiblities[:, i][:, np.newaxis] * diff).T, diff) / responsiblities[:, i].sum()\n",
    "    return covs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the new weights are found as the average of the responsibilities, \n",
    "\n",
    "$$\n",
    "\\mu_k = \\frac{\\sum_{i=1}^{N}\\gamma_{i, k}}{N}. \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_update(responsiblities):\n",
    "    \"\"\"\n",
    "    Compute the new weights given the responsibilities.\n",
    "    \n",
    "    :param responsiblities: The responsibilities for the data points.\n",
    "    \n",
    "    :return: The new weights.\n",
    "    \"\"\"\n",
    "    return responsiblities.mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can put this together in a loop, with initial weights of one-third each. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.ones(3) / 3\n",
    "\n",
    "for i in range(3):\n",
    "    responsiblities = expectation(data.values, mvs, weights)\n",
    "    means = means_update(responsiblities, data.values)\n",
    "    covs = covariances_update(responsiblities, data.values, means)\n",
    "    weights = weights_update(responsiblities)\n",
    "    mvs = [multivariate_normal(mean=mean, cov=cov) for mean, cov in zip(means, covs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After three iterations of the algorithm, let's see how the distributions look. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for i, ax in enumerate(ax):\n",
    "    ax.contour(X1, X2, mvs[i].pdf(np.dstack((X1, X2))), cmap='Grays')\n",
    "    ax.scatter(data['x1'], data['x2'])\n",
    "    ax.set_title(f'Cluster {i}')\n",
    "    ax.set_xlabel('x1')\n",
    "    ax.set_ylabel('x2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that even after a small number of iterations, we describe the clusters well. \n",
    "Now, we will introduce a convergence criteria that will stop the iterative process. \n",
    "In this case, we will choose the point when the parameters stop changing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means_list = [means]\n",
    "covs_list = [covs]\n",
    "weights_list = [weights]\n",
    "\n",
    "for i in range(100):\n",
    "    responsiblities = expectation(data.values, mvs, weights)\n",
    "    means_list.append(means_update(responsiblities, data.values))\n",
    "    covs_list.append(covariances_update(responsiblities, data.values, means_list[-1]))\n",
    "    weights_list.append(weights_update(responsiblities))\n",
    "    mvs = [multivariate_normal(mean=mean, cov=cov) for mean, cov in zip(means_list[-1], covs_list[-1])]\n",
    "    if np.allclose(means_list[-1], means_list[-2]) and np.allclose(covs_list[-1], covs_list[-2]) and np.allclose(weights_list[-1], weights_list[-2]):\n",
    "        break\n",
    "print(f'Stopped after {i} iterations')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only an additional seven iterations were necessary for convergence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for i, ax in enumerate(ax):\n",
    "    ax.contour(X1, X2, mvs[i].pdf(np.dstack((X1, X2))), cmap='Grays')\n",
    "    ax.scatter(data['x1'], data['x2'])\n",
    "    ax.set_title(f'Cluster {i}')\n",
    "    ax.set_xlabel('x1')\n",
    "    ax.set_ylabel('x2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this, we can generate cluster labels for each data point. \n",
    "The cluster with the highest responsibility for a given data point is the one to which the data point is assigned. \n",
    "We can find this with the `np.argmax` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_labels = np.argmax(responsiblities, axis=1)\n",
    "\n",
    "data_label = data.copy()\n",
    "data_label['labels'] = cluster_labels\n",
    "fig, ax = plt.subplots()\n",
    "sns.scatterplot(x='x1', y='x2', data=data_label, hue='labels', ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can treat the responsibilities of each data point for each cluster as a probability. \n",
    "This probability describes how likely the data point is associated with a given cluster. \n",
    "This means that we can visualise the probability that a data point is a member of a cluster with the size of the marker. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 10 * responsiblities.max(axis=1) ** 2\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(data['x1'], data['x2'], c=cluster_labels, s=size)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear that the points closer to the centre of each distribution, where the probability density function is greater, are more *likely* to be members of a given cluster. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "special-topics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
