{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DBSCAN\n",
    "\n",
    "Density-Based Spatial Clustering of Applications with Noise, or DBSCAN for short, is an algorithm that groups data points in areas of high density and then identifies data in lower-density regions as noise. \n",
    "Similar to the purely hierarchical clustering method discussed previously, this does not assume the shape of the clusters. \n",
    "Unlike *k*-means and GMMs, which assume spheres and ellipses, respectively. \n",
    "Additionally, it does not assume some given cluster number. \n",
    "\n",
    "To discuss the DBSCAN algorithm, let's look at a [slightly different dataset](../data/moons_blobs.txt). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = np.loadtxt('../data/moons_blobs.txt') \n",
    "\n",
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "ax.plot(*X.T, '.')\n",
    "ax.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there are four clusters, two of which are intersecting moons and two are circular. \n",
    "First, we can see how GMMs would cope. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "gmm = GaussianMixture(n_components=4, random_state=42)\n",
    "gmm.fit(X)\n",
    "label = gmm.predict(X)\n",
    "prob = gmm.predict_proba(X)\n",
    "size = 50 * prob.max(1) ** 2\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "ax.scatter(*X.T, s=size, c=label)\n",
    "ax.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear that even though the Gaussian mixture model is told that there are 4 clusters, it fails to find the moons, given they cannot be described with an elliptical model. \n",
    "However, can the DBSCAN approach improve on this? \n",
    "\n",
    "## DBSCAN Algorithm\n",
    "\n",
    "We will need to know the distance between all the points. \n",
    "So, let's generate that in the same fashion as previously. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "distances = squareform(pdist(X))\n",
    "np.fill_diagonal(distances, np.inf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the algorithm's start, all data points are classified as *unvisited*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visited = [False] * X.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will then visit the first data point and update the visited status. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "visited[0] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For that first data point, we want to find all of the other points that are less than some distance `eps` from it; these are the neighbours of the data point. \n",
    "This `eps` is one of the hyperparameters of the DBSCAN algorithm; for this example, we will use a value of 0.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 0.1\n",
    "np.where(distances[0] < eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````{margin}\n",
    "```{note}\n",
    "This classification as noise may change in the future. \n",
    "```\n",
    "````\n",
    "If the data point has fewer than `min_samples` (the second hyperparameter, here we use 5), then it is identified as noise. \n",
    "We will do this by making the value of a `labels` array -1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_samples = 5\n",
    "labels = np.zeros(X.shape[0], dtype=int)\n",
    "\n",
    "if (distances[0] < eps).sum() < min_samples:\n",
    "    labels[0] = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, if the point has more than `min_samples` neighbours, we classify it as a core point and expand the cluster around it. \n",
    "Data point 338 has many neighbours. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(distances[338] < eps).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A core point is assigned a `cluster_id`, which starts from 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_id = 0\n",
    "\n",
    "labels[338] = cluster_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then, for each point in the neighbour of a data point, we perform the following: \n",
    "- If the point is unvisited, its neighbours are found and marked as visited, and if it has more than the `min_samples` of neighbours, then these are included as neighbours of the core point. \n",
    "- If the point has been visited and is currently defined as noise, the label is changed to that of the core point.\n",
    "\n",
    "This is shown for data point 338 below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbours = np.where(distances[338] < eps)[0]\n",
    "\n",
    "j = 0\n",
    "while j < neighbours.size:\n",
    "    current = neighbours[j]\n",
    "    if not visited[current]:\n",
    "        visited[current] = True\n",
    "        new_neighbors = np.where(distances[current] <= eps)[0]\n",
    "        if new_neighbors.size >= min_samples:\n",
    "            neighbours = np.concatenate([neighbours, new_neighbors])\n",
    "    if labels[current] == -1:\n",
    "        labels[current] = cluster_id\n",
    "    j += 1\n",
    "cluster_id += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this has led to many particles being visited. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(visited)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To put the algorithm together, we will reset some of the bookkeeping. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "visited = [False] * X.shape[0]\n",
    "labels = np.zeros(X.shape[0], dtype=int) - 1\n",
    "cluster_id = 0\n",
    "\n",
    "for i in range(X.shape[0]):\n",
    "    if not visited[i]:\n",
    "        visited[i] = True\n",
    "        neighbours = np.where(distances[i] <= eps)[0]\n",
    "        if neighbours.size < min_samples:\n",
    "            labels[i] = -1\n",
    "        else:\n",
    "            labels[i] = cluster_id\n",
    "            j = 0\n",
    "            while j < neighbours.size:\n",
    "                current = neighbours[j]\n",
    "                if not visited[current]:\n",
    "                    visited[current] = True\n",
    "                    new_neighbors = np.where(distances[current] <= eps)[0]\n",
    "                    if new_neighbors.size >= min_samples:\n",
    "                        neighbours = np.concatenate([neighbours, new_neighbors])\n",
    "                if labels[current] == -1:\n",
    "                    labels[current] = cluster_id\n",
    "                j += 1\n",
    "            cluster_id += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the current hyperparameter values, the DBSCAN algorithm has produced 12 clusters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And identified 134 of the 400 data points as noise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(labels == -1).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try to visualise this, but there probably isn't enough colour fidelity in the `matplotlib` standard plotting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "ax.scatter(*X.T, c=labels)\n",
    "ax.axis('equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HDBSCAN\n",
    "\n",
    "Before we leave clustering behind, we will look at one final approach, Hierarchical DBSCAN{cite}`Campello2013`. \n",
    "HDBSCAN brings the hierarchical clustering approach together with DBSCAN.\n",
    "One of the significant benefits of HDBSCAN over DBSCAN is that it is not necessary to define the hyperparameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import HDBSCAN\n",
    "\n",
    "hdbscan = HDBSCAN()\n",
    "label = hdbscan.fit_predict(X)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "ax.scatter(*X.T, c=label)\n",
    "ax.axis('equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A na√Øve usage of the `scipy.cluster.HBDSCAN` method can perfectly capture the data.\n",
    "\n",
    "This isn't to say that HDBSCAN is perfect, but there is still room for other clustering methods.\n",
    "This is not least due to the high computational cost of HDBSCAN compared to the more straightforward approaches. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "special-topics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
