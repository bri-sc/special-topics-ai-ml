{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DBSCAN\n",
    "\n",
    "Density-Based Spatial Clustering of Applications with Noise or DBSCAN for short is an algorithm that groups data points together that are in areas of high density and then identify data in lower density regions as noise. \n",
    "Similar to the purely hierarchical clustering method discussed previously, this makes no assumption about the shape of the clusters. \n",
    "Unlike *k*-means and GMMs, which assume spheres and ellipses, respectively. \n",
    "Additionally, it does not assume some given cluster number. \n",
    "\n",
    "To discuss the DBSCAN algorithm, let's look at a slightly different dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons, make_blobs\n",
    "\n",
    "X1, y1 = make_moons(n_samples=200, noise=0.075, random_state=42)\n",
    "X2, y2 = make_blobs(n_samples=200, centers=2, random_state=42) \n",
    "X2 /= np.array([10, 10])\n",
    "X2 += np.array([1, 1])\n",
    "\n",
    "X = np.vstack([X1, X2])\n",
    "y = np.hstack([y1, y2])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "ax.plot(*X.T, '.')\n",
    "ax.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there are four cluster, two of which are intersceting moons and two are circular. \n",
    "First, we can see how GMMs would cope. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "gmm = GaussianMixture(n_components=4, random_state=42)\n",
    "gmm.fit(X)\n",
    "label = gmm.predict(X)\n",
    "prob = gmm.predict_proba(X)\n",
    "size = 50 * prob.max(1) ** 2\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "ax.scatter(*X.T, s=size, c=label)\n",
    "ax.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear, that even though the Gaussian mixture model is told that there are 4 clusters, it fails to find the moons, given they cannot be described with an ellipitical model. \n",
    "However, can the DBSCAN approach improve on this. \n",
    "\n",
    "## DBSCAN Algorithm\n",
    "\n",
    "At the start of the algorithm, all of the data points are classified as *unvisited*. \n",
    "Additionally, we will need to know the distance between all the points. \n",
    "So let's generate that in the same fashion as previously. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "visited = [False] * X.shape[0]\n",
    "\n",
    "distances = squareform(pdist(X))\n",
    "np.fill_diagonal(distances, np.inf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will then visit the first data point, and update the visited status. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "visited[0] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For that first data point, we want to find all of the other points that are less then some distance `eps` from it, these are the neighbours of the data point. \n",
    "This `eps` is one of the hyperparameters of the DBSCAN algorithm, for this example, we will use a value of 0.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 0.1\n",
    "np.where(distances[0] < eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````{margin}\n",
    "```{note}\n",
    "This classification an noise may change in the future. \n",
    "```\n",
    "````\n",
    "If the data point has fewer than `min_samples` (the second hyperparameter, here we use 5), then the point is identified a noise. \n",
    "We will do this by making the value of a `labels` array -1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_samples = 5\n",
    "labels = np.zeros(X.shape[0], dtype=int)\n",
    "\n",
    "if (distances[0] < eps).sum() < min_samples:\n",
    "    labels[0] = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, if the point has more than `min_samples` neighbours, we classify this as a core point and begin to expand the cluster around it. \n",
    "Data point 338 has many neighbours. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(distances[338] < eps).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A core point is assigned a `cluster_id`, which starts from 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_id = 0\n",
    "\n",
    "labels[338] = cluster_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then for each point in the neighbour of a data point, we perform the following: \n",
    "- If the point is unvisited, its neighbours are found and marked as visited and if it has more then the `min_samples` of neighbours then these are included as neighbours of the core point. \n",
    "- If the point has been visited and it currently defined as noise, the label is changed to that of the core point.\n",
    "\n",
    "This is shown for data point 338 below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbours = np.where(distances[338] < eps)[0]\n",
    "\n",
    "j = 0\n",
    "while j < neighbours.size:\n",
    "    current = neighbours[j]\n",
    "    if not visited[current]:\n",
    "        visited[current] = True\n",
    "        new_neighbors = np.where(distances[current] <= eps)[0]\n",
    "        if new_neighbors.size >= min_samples:\n",
    "            neighbours = np.concatenate([neighbours, new_neighbors])\n",
    "    if labels[current] == -1:\n",
    "        labels[current] = cluster_id\n",
    "    j += 1\n",
    "cluster_id += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this has lead to a large number of particles being visited. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(visited)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To put the algorithm together, we will reset some of the bookkeeping. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "visited = [False] * X.shape[0]\n",
    "labels = np.zeros(X.shape[0], dtype=int) - 1\n",
    "cluster_id = 0\n",
    "\n",
    "for i in range(X.shape[0]):\n",
    "    if not visited[i]:\n",
    "        visited[i] = True\n",
    "        neighbours = np.where(distances[i] <= eps)[0]\n",
    "        if neighbours.size < min_samples:\n",
    "            labels[i] = -1\n",
    "        else:\n",
    "            labels[i] = cluster_id\n",
    "            j = 0\n",
    "            while j < neighbours.size:\n",
    "                current = neighbours[j]\n",
    "                if not visited[current]:\n",
    "                    visited[current] = True\n",
    "                    new_neighbors = np.where(distances[current] <= eps)[0]\n",
    "                    if new_neighbors.size >= min_samples:\n",
    "                        neighbours = np.concatenate([neighbours, new_neighbors])\n",
    "                if labels[current] == -1:\n",
    "                    labels[current] = cluster_id\n",
    "                j += 1\n",
    "            cluster_id += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using these values of the hyperparameters, the DBSCAN algorithm has produced 12 clusters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And identified 134 of the 400 data points as noise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(labels == -1).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We try to visualise this but there probably isn't enough colour fidelity in the `matplotlib` standard plotting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "ax.scatter(*X.T, c=labels)\n",
    "ax.axis('equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HDBSCAN\n",
    "\n",
    "Before we leave clustering behind, we will look at one final approach, Hierarchical DBSCAN{cite}`Campello2013`. \n",
    "HDBSCAN brings the hierarchical clustering approach together with DBSCAN.\n",
    "One of the major benefits of HDBSCAN over DBSCAN is that it is not necessary to define the hyperparameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import HDBSCAN\n",
    "\n",
    "hdbscan = HDBSCAN()\n",
    "label = hdbscan.fit_predict(X)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "ax.scatter(*X.T, c=label)\n",
    "ax.axis('equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A na√Øve usage of the `scipy.cluster.HBDSCAN` method is able to perfectly capture the data.\n",
    "\n",
    "This isn't to say that HDBSCAN is perfect, and there is still a place for the other clustering methods.\n",
    "Not least due to the high computational cost of HDBSCAN compared to the more straight-forward approaches. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "special-topics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
