{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Clustering\n",
    "\n",
    "Building a hierarchy of clusters can be beneficial to understand similarity between data points. \n",
    "Hierarchical clustering is a class of clustering methods that do not require the number of clusters at the time of analysis but instead allow the flexibility to chose the cluster numbers after the analysis is complete. \n",
    "Additionally, some argue that greater insight can be obtained from hierarchical clustering as this allows nested clusters to be found, revealing more information about the relationships between data points. \n",
    "In notable difference between hierarchical clustering and the previous clustering approaches discussed is that there is no underlaying statistical model in hierarchical clustering as there is in the other approaches; i.e., we can think of *k*-means a non-covariant equivalent to Gaussian mixture models. \n",
    "\n",
    "Hierarchical clustering comes in two flavours: \n",
    "- Agglomerative (or bottom-up) clustering starts with each data point as its own cluster and then merges nearby clusters together. That approach is quite natural and intuitive. \n",
    "- Divisive (or top-down) clustering does the opposite, recursively splitting clusters until some stopping criterion is reached. This is a less common approach due to increased computational complexity. \n",
    "\n",
    "## How Agglomerative Clustering Works\n",
    "\n",
    "Here, we will look in detail at agglomerative clustering, and build a simple example. \n",
    "For this, we will use a subset of the dataset that *k*-means could not cluster correctly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "\n",
    "n_samples = 50\n",
    "\n",
    "X, y = datasets.make_blobs(n_samples=n_samples, random_state=170)\n",
    "transformation = [[0.6, -0.6], [-0.4, 0.8]]\n",
    "X_aniso = np.dot(X, transformation)\n",
    "data = pd.DataFrame(X_aniso, columns=['x1', 'x2'])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "sns.scatterplot(x='x1', y='x2', data=data, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to compute the distance matrix, all pairwise distances, between data points. \n",
    "We can achieve this with NumPy and using the `np.newaxis` functionality but here we use functionality from the [`scipy.spatial.distance`](https://docs.scipy.org/doc/scipy-1.15.0/reference/spatial.distance.html) library. We also fill the diagonal with infinity values, as these represent the distance between a data point and itself, which is 0. \n",
    "````{margin}\n",
    "```{note}\n",
    "`pdist` stands for *pairwise* distances and `squareform` produces an $N\\timesN$ matrix, where $N$ is the number of data points.\n",
    "```\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "distances = squareform(pdist(X_aniso))\n",
    "np.fill_diagonal(distances, np.inf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````{admonition} A Comment on Distance\n",
    ":class: tip\n",
    "The distance that we traditionally calculate is knowns as the Euclidean distance between two points. \n",
    "But in machine learning methods, there are other distance metrics that can be used. \n",
    "{numref}`distances` shows some of the other distances that are used. \n",
    "```{figure} ../images/distances.png\n",
    "---\n",
    "name: distances\n",
    "height: 100%\n",
    "---\n",
    "A diagram presenting some different distances that may be used. Modified from [Maatren Grootendorst](https://towardsdatascience.com/9-distance-measures-in-data-science-918109d069fa).\n",
    "```\n",
    "It is important that the distance used has a logical rational behind it. \n",
    "A full list of the distances that the `scipy.spatial.distance.pdist` function can compute can be found in [the documentation](https://docs.scipy.org/doc/scipy-1.15.0/reference/generated/scipy.spatial.distance.pdist.html). \n",
    "````\n",
    "\n",
    "Next, we start things off by creating a dictionary of clusters, where each point is its own cluster and create an empty list that we will use to store the *dendrogram*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = {i: [i] for i in range(X_aniso.shape[0])}\n",
    "dendrogram = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to find the two particles that are the closest to each other. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two data points that are only 0.062 away from each other. \n",
    "But, which data points are they?\n",
    "We can find this by comparing the minimum of the newly created `active_distances` array with the `distances` array. \n",
    "This slightly convoluted approach is necessary, as later we will remove rows and columns from the `active_distances` but to keep track of the indices they won't be removed from `distances`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_distances = distances.copy()\n",
    "minimum_pair = np.array(np.where(distances == active_distances.min()))[:, 0]\n",
    "minimum_pair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two closest points are data point 0 and data point 17. \n",
    "Let's check this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "sns.scatterplot(x='x1', y='x2', data=data, ax=ax)\n",
    "ax.plot(X_aniso[[minimum_pair], 0], X_aniso[[minimum_pair], 1], 'ks')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks correct, so we can now create our first cluster and add it to the dendrogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dendrogram.append([minimum_pair[0], minimum_pair[1], active_distances.min(), len(clusters[minimum_pair[0]]) + len(clusters[minimum_pair[1]])])\n",
    "new_cluster = clusters[minimum_pair[0]] + clusters[minimum_pair[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `new_cluster` replaces the first point in the dictionary and we delete the second, this is important bookeeping. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "del clusters[minimum_pair[1]]\n",
    "clusters[minimum_pair[0]] = new_cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we update the distance matrix. \n",
    "This involves finding the minimum distance between the two data points in the cluster and all the other data points (or clusters). \n",
    "This minimum distance is used to update the distance matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in clusters.keys():\n",
    "    distances[minimum_pair[0], k] = np.min([distances[p1, p2] for p1 in clusters[minimum_pair[0]] for p2 in clusters[k]])\n",
    "    distances[k, minimum_pair[0]] = distances[minimum_pair[0], k]\n",
    "distances[minimum_pair[0], minimum_pair[0]] = np.inf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step to update the distance matrix is to remove the rows and columns associated with the second cluster of the minimum pair. \n",
    "This is achieved by making a nested list of the indices that are in clusters, and flattening this, the flattened list is then used in the `np.delete` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_remove = [i for sublist in [v[1:] for k, v in clusters.items() if len(v) > 1] for i in sublist]\n",
    "\n",
    "active_distances = distances.copy()\n",
    "active_distances = np.delete(active_distances, to_remove, axis=0)\n",
    "active_distances = np.delete(active_distances, to_remove, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, this convoluted approach is necessary for when there are clusters of clusters. \n",
    "\n",
    "```{admonition} Linkages\n",
    ":class: tip\n",
    "Above we use what is known as a *single linkage* criterion when we took the minimum distance of the points in the cluster with the other points. \n",
    "This is not the only approach to describe our clusters. \n",
    "Other linkages include:\n",
    "- Complete linkage: where the maximum distance of the points in the cluster is used. \n",
    "- Average linkage: where the average distance is used. \n",
    "- Ward's linkage{cite}`Ward1963`: looks to merge the clusters by minimising the total within-cluster variance. \n",
    "```\n",
    "\n",
    "The process is then repeated until there is only a single cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_list = [clusters.copy()]\n",
    "while len(clusters) > 1:\n",
    "    minimum_pair = np.array(np.where(distances == active_distances.min()))[:, 0]\n",
    "    dendrogram.append([minimum_pair[0], minimum_pair[1], active_distances.min(), len(clusters[minimum_pair[0]]) + len(clusters[minimum_pair[1]])])\n",
    "    new_cluster = clusters[minimum_pair[0]] + clusters[minimum_pair[1]]\n",
    "    del clusters[minimum_pair[1]]\n",
    "    clusters[minimum_pair[0]] = new_cluster \n",
    "    clusters_list.append(clusters.copy())\n",
    "    for k in clusters.keys():\n",
    "        distances[minimum_pair[0], k] = np.min([distances[p1, p2] for p1 in clusters[minimum_pair[0]] for p2 in clusters[k]])\n",
    "        distances[k, minimum_pair[0]] = distances[minimum_pair[0], k]\n",
    "    distances[minimum_pair[0], minimum_pair[0]] = np.inf\n",
    "    to_remove = [i for sublist in [v[1:] for k, v in clusters.items() if len(v) > 1] for i in sublist]\n",
    "    active_distances = distances.copy()\n",
    "    active_distances = np.delete(active_distances, to_remove, axis=0)\n",
    "    active_distances = np.delete(active_distances, to_remove, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation of Hierarchical Clustering\n",
    "\n",
    "Now that we have performed the hierarchical clustering, it is time to visualise the results. \n",
    "Let's look first of all at the data, with colours based on the clusters that have been assigned.\n",
    "We can do this for any number of clusters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for i, clusters in enumerate([2, 3, 4]):\n",
    "    for k, v in clusters_list[-clusters].items():\n",
    "        ax[i].scatter(X_aniso[v, 0], X_aniso[v, 1], label=k)\n",
    "        ax[i].set_xlabel('x1')\n",
    "        ax[i].set_ylabel('x2')\n",
    "        ax[i].set_title(f'Clusters: {clusters}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsurprisingly, three clusters are the best representation of the data.\n",
    "However, four is also quite convincing. \n",
    "\n",
    "The next visualisation we can use, can help us to understand the groupings in our hierarchical clustering. \n",
    "This is called a *dendrogram* and represents the connectivity of the clusters at each step of the algorithm.\n",
    "\n",
    "````{margin}\n",
    "```{note}\n",
    "To plot the dendrogram, we use a function from `scipy.cluster.hierarchy`. \n",
    "However, the expected input for this function is slightly different to the dendrogram object we created in the algorithm.\n",
    "Therefore, there is a `helper` module that can be used to convert to the compatible structure. \n",
    "The `helper` module can be downloaded [here](helper.py).\n",
    "```\n",
    "`````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram as dendrogram_plot\n",
    "from helper import dendrogram_convert\n",
    "\n",
    "dg = dendrogram_plot(dendrogram_convert(dendrogram))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dendrogram lets us see the connectivity, but is also powerful to determine how many clusters should be used. \n",
    "The areas with the largest vertical distance represent places where clusters that are very far apart have been linked. \n",
    "Hence, the large distance bewteen where there are four clusters (around 1 on the *y*-axis) and where there are five clusters (around 0.87 on the *y*-axis).\n",
    "\n",
    "Now that we understand the basic algorithm behind hierarchical clustering, we can start to look at using more complex implementations of it. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "special-topics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
