{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Clustering\n",
    "\n",
    "Building a hierarchy of clusters can be beneficial to understanding the similarity between data points. \n",
    "Hierarchical clustering is a class of clustering methods that do not require the number of clusters at the time of analysis but instead allow the flexibility to choose the cluster numbers after the analysis is complete. \n",
    "Additionally, some argue that greater insight can be obtained from hierarchical clustering, which allows nested clusters to be found, revealing more information about the relationships between data points. \n",
    "A notable difference between hierarchical clustering and the previous clustering approaches discussed is that there is no underlying statistical model in hierarchical clustering as in the other approaches; i.e., we can think of *k*-means as a non-covariant equivalent to Gaussian mixture models. \n",
    "\n",
    "Hierarchical clustering comes in two flavours: \n",
    "- Agglomerative (or bottom-up) clustering starts with each data point as its own cluster and then merges nearby clusters. That approach is quite natural and intuitive. \n",
    "- Divisive (or top-down) clustering does the opposite, recursively splitting clusters until some stopping criterion is reached. This is a less common approach due to increased computational complexity. \n",
    "\n",
    "## How Agglomerative Clustering Works\n",
    "\n",
    "Here, we will look at agglomerative clustering in detail and build a simple example. \n",
    "We will use a [subset of the dataset that *k*-means could not cluster correctly](../data/skew-small.csv). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "\n",
    "data = pd.read_csv('../data/skew-small.csv')\n",
    "X = data.values\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "sns.scatterplot(x='x1', y='x2', data=data, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to compute the distance matrix, all pairwise distances, between data points. \n",
    "We can achieve this with NumPy and using the `np.newaxis` functionality, but here we use functionality from the [`scipy.spatial.distance`](https://docs.scipy.org/doc/scipy-1.15.0/reference/spatial.distance.html) library. \n",
    "We also fill the diagonal with infinity values, representing the distance between a data point and itself, which is 0. \n",
    "````{margin}\n",
    "```{note}\n",
    "`pdist` stands for *pairwise* distances and `squareform` produces an $N\\timesN$ matrix, where $N$ is the number of data points.\n",
    "```\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "distances = squareform(pdist(X))\n",
    "np.fill_diagonal(distances, np.inf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````{admonition} A Comment on Distance\n",
    ":class: tip\n",
    "The distance we traditionally calculate is the Euclidean distance between two points. \n",
    "However, in machine learning methods, other distance metrics can be used. \n",
    "{numref}`distances` shows some of the other distances that are used. \n",
    "```{figure} ../images/distances.png\n",
    "---\n",
    "name: distances\n",
    "height: 100%\n",
    "---\n",
    "A diagram presenting some different distances that may be used. Modified from [Maatren Grootendorst](https://towardsdatascience.com/9-distance-measures-in-data-science-918109d069fa).\n",
    "```\n",
    "The distance used must have a logical rationale behind it. \n",
    "A complete list of the distances that the `scipy.spatial.distance.pdist` function can compute can be found in [the documentation](https://docs.scipy.org/doc/scipy-1.15.0/reference/generated/scipy.spatial.distance.pdist.html). \n",
    "````\n",
    "\n",
    "Next, we start by creating a dictionary of clusters, where each point is its own cluster and create an empty list that we will use to store the *dendrogram*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = {i: [i] for i in range(X.shape[0])}\n",
    "dendrogram = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to find the two particles closest to each other. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two data points are only 0.062 away from each other. \n",
    "But which data points are they?\n",
    "We can find this by comparing the minimum of the newly created `active_distances` array with the `distances` array. \n",
    "This slightly convoluted approach is necessary, as we will remove rows and columns from the `active_distances` later, but to keep track of the indices, they won't be removed from `distances`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_distances = distances.copy()\n",
    "minimum_pair = np.array(np.where(distances == active_distances.min()))[:, 0]\n",
    "minimum_pair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two closest points are data point 0 and data point 17. \n",
    "Let's check this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "sns.scatterplot(x='x1', y='x2', data=data, ax=ax)\n",
    "ax.plot(X[[minimum_pair], 0], X[[minimum_pair], 1], 'ks')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks correct, so we can now add our first cluster to the dendrogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dendrogram.append([minimum_pair[0], minimum_pair[1], active_distances.min(), len(clusters[minimum_pair[0]]) + len(clusters[minimum_pair[1]])])\n",
    "new_cluster = clusters[minimum_pair[0]] + clusters[minimum_pair[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `new_cluster` replaces the first point in the dictionary, and we delete the second; this is essential bookkeeping. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "del clusters[minimum_pair[1]]\n",
    "clusters[minimum_pair[0]] = new_cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we update the distance matrix. \n",
    "This involves finding the minimum distance between the two data points in the cluster and all the other data points (or clusters). \n",
    "This minimum distance is used to update the distance matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in clusters.keys():\n",
    "    distances[minimum_pair[0], k] = np.min([distances[p1, p2] for p1 in clusters[minimum_pair[0]] for p2 in clusters[k]])\n",
    "    distances[k, minimum_pair[0]] = distances[minimum_pair[0], k]\n",
    "distances[minimum_pair[0], minimum_pair[0]] = np.inf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step to update the distance matrix is to remove the rows and columns associated with the second cluster of the minimum pair. \n",
    "This is achieved by making a nested list of the indices in clusters and flattening this; the flattened list is then used in the `np.delete` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_remove = [i for sublist in [v[1:] for k, v in clusters.items() if len(v) > 1] for i in sublist]\n",
    "\n",
    "active_distances = distances.copy()\n",
    "active_distances = np.delete(active_distances, to_remove, axis=0)\n",
    "active_distances = np.delete(active_distances, to_remove, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, this convoluted approach is necessary for clusters of clusters to be accounted for. \n",
    "\n",
    "```{admonition} Linkages\n",
    ":class: tip\n",
    "Above, we use what is known as a *single linkage* criterion when we take the minimum distance of the points in the cluster from the other points. \n",
    "This is not the only approach to describe our clusters. \n",
    "Other linkages include:\n",
    "- Complete linkage: where the maximum distance of the points in the cluster is used. \n",
    "- Average linkage: where the average distance is used. \n",
    "- Ward's linkage{cite}`Ward1963`: looks to merge the clusters by minimising the total within-cluster variance. \n",
    "```\n",
    "\n",
    "The process is then repeated until there is only a single cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_list = [clusters.copy()]\n",
    "while len(clusters) > 1:\n",
    "    minimum_pair = np.array(np.where(distances == active_distances.min()))[:, 0]\n",
    "    dendrogram.append([minimum_pair[0], minimum_pair[1], active_distances.min(), len(clusters[minimum_pair[0]]) + len(clusters[minimum_pair[1]])])\n",
    "    new_cluster = clusters[minimum_pair[0]] + clusters[minimum_pair[1]]\n",
    "    del clusters[minimum_pair[1]]\n",
    "    clusters[minimum_pair[0]] = new_cluster \n",
    "    clusters_list.append(clusters.copy())\n",
    "    for k in clusters.keys():\n",
    "        distances[minimum_pair[0], k] = np.min([distances[p1, p2] for p1 in clusters[minimum_pair[0]] for p2 in clusters[k]])\n",
    "        distances[k, minimum_pair[0]] = distances[minimum_pair[0], k]\n",
    "    distances[minimum_pair[0], minimum_pair[0]] = np.inf\n",
    "    to_remove = [i for sublist in [v[1:] for k, v in clusters.items() if len(v) > 1] for i in sublist]\n",
    "    active_distances = distances.copy()\n",
    "    active_distances = np.delete(active_distances, to_remove, axis=0)\n",
    "    active_distances = np.delete(active_distances, to_remove, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation of Hierarchical Clustering\n",
    "\n",
    "Now that we have performed the hierarchical clustering, it is time to visualise the results. \n",
    "First of all, let's look at the data with colours based on the clusters that have been assigned.\n",
    "We can do this for any number of clusters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for i, clusters in enumerate([2, 3, 4]):\n",
    "    for k, v in clusters_list[-clusters].items():\n",
    "        ax[i].scatter(X[v, 0], X[v, 1], label=k)\n",
    "        ax[i].set_xlabel('x1')\n",
    "        ax[i].set_ylabel('x2')\n",
    "        ax[i].set_title(f'Clusters: {clusters}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsurprisingly, three clusters are the best representation of the data.\n",
    "However, four is also quite convincing. \n",
    "\n",
    "The following visualisation we can use can help us understand the groupings in our hierarchical clustering. \n",
    "This is called a *dendrogram* and represents the connectivity of the clusters at each step of the algorithm.\n",
    "\n",
    "````{margin}\n",
    "```{note}\n",
    "We use a function from `scipy.cluster.hierarchy` to plot the dendrogram. \n",
    "However, this function's expected input is slightly different from the dendrogram object we created in the algorithm.\n",
    "Therefore, a `helper` module can be used to convert to a compatible structure. \n",
    "The `helper` module can be downloaded [here](helper.py).\n",
    "```\n",
    "`````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram as dendrogram_plot\n",
    "from helper import dendrogram_convert\n",
    "\n",
    "dg = dendrogram_plot(dendrogram_convert(dendrogram))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dendrogram lets us see the connectivity but can also be used to determine how many clusters are meaningful. \n",
    "The areas with the largest vertical distance represent places where clusters very far apart have been linked. \n",
    "Hence, there is a large distance between four clusters (around 1 on the *y*-axis) and five clusters (around 0.87 on the *y*-axis). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "special-topics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
