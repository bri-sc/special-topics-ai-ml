{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *k*-Means Algorithm\n",
    "\n",
    "The EM algorithm for *k*-means has the following procedure: \n",
    "1. Guess some initial cluster centres.\n",
    "2. Repeat the following until convergence is reached:\n",
    "    1. E-step: Assign points to the nearest cluster centre.\n",
    "    2. M-step: Update the cluster centres with the mean position.\n",
    "\n",
    "The E-step updates our expectation of which cluster each of the points belongs to, while the M-step maximises some fitness function that defines the cluster centre locations. \n",
    "Each repetition of the E-M loop will always result in a better estimate of the cluster characteristics. \n",
    "\n",
    "Let's try and write some code to perform this algorithm ourselves. \n",
    "First, we need some data to cluster, for this, we will use the `scikit-learn` method to produce random blobs of data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X, y_true = make_blobs(n_samples=300, centers=4, cluster_std=1, random_state=0)\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "ax.plot(X[:, 0], X[:, 1], '.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try and identify four clusters in this data, therefore, we set the following. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a starting position, let's select four of the points randomly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "i = np.random.randint(0, X.shape[0], size=4)\n",
    "centres = X[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our initial guess is shown below with orange squares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "ax.plot(X[:, 0], X[:, 1], '.')\n",
    "ax.plot(centres[:, 0], centres[:, 1], 's')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is quite clear that they do **not** represent the arithmetic mean of any of the data. \n",
    "The second step (the first of the E-M loop) is to assign all of the points to their nearest cluster centre. \n",
    "This can be achieved very efficiently by using NumPy and array broadcasting. \n",
    "First, we compute the vector from each of the data, `X`, to each of the current cluster centres. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = X[:, np.newaxis] - centres\n",
    "vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the shape of this array is `(number_of_data, number_of_clusters, dimensions)`. \n",
    "The magnitude of the vector can describe the distance from the data to the current centres, we want the magnitude along the dimensions axis of the array (the final axis, hence `-1`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "magnitudes = np.linalg.norm(vectors, axis=-1)\n",
    "magnitudes.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have the magnitude of the vector from each data point to each of the cluster centres. \n",
    "We want to know which centre each datapoint is closest to, for this, we can use the `argmin()` method of the NumPy array. \n",
    "This returns the index of the minimum value (the method `min()` will return the actual value, but we aren't actually interested in this).\n",
    "The `argmin()` method should only be run on the number of clusters axis so that we end up with an array of 300 indices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = magnitudes.argmin(axis=1)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This array, `labels`, is the assignment of each of the data points to the closest cluster centres, completing the E-step in the E-M loop. \n",
    "Now, we need to update the cluster centres, with the new arithmetic mean of the data that are assigned to it. \n",
    "The data can be split into groups using logical slicing, i.e., to get all of the values with the label `0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[labels == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, are all of the datapoints in associated with the first cluster centre, and the mean (along the datapoint axis can be found as)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[labels == 0].mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And it is possible to perform this for all four of the clusters with simple list comprehension. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_centres = np.array([X[labels == i].mean(axis=0) for i in range(n_clusters)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new centres can be visualised as shown below (this time with triangles). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "ax.plot(X[:, 0], X[:, 1], '.')\n",
    "ax.plot(centres[:, 0], centres[:, 1], 's')\n",
    "ax.plot(new_centres[:, 0], new_centres[:, 1], '^')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, to update the centres, we overwrite the `centres` object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "centres = new_centres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This process should be completed until the centres do not change meaning that we can use an iterative approach. \n",
    "See the code cell below, which loops over this process until the is no change in the centres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "import numpy as np\n",
    "\n",
    "X, y_true = make_blobs(n_samples=300, centers=4, cluster_std=1, random_state=1)\n",
    "n_clusters = 4\n",
    "i = np.random.randint(0, X.shape[0], size=n_clusters)\n",
    "centres = X[i]\n",
    "diff = np.inf\n",
    "while np.sum(diff) > 0.00000001:\n",
    "    vectors = X[:, np.newaxis] - centres\n",
    "    magnitudes = np.linalg.norm(vectors, axis=-1)\n",
    "    labels = magnitudes.argmin(axis=1)\n",
    "    new_centres = np.array([X[labels == i].mean(axis=0) for i in range(n_clusters)])\n",
    "    diff = np.abs(centres - new_centres)\n",
    "    centres = new_centres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We show the results below, with the different clusters identified by colour and the cluster centres marked with a black square. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "for i in range(n_clusters):\n",
    "    ax.plot(X[labels == i][:, 0], X[labels == i][:, 1], '.')\n",
    "ax.plot(centres[:, 0], centres[:, 1], 'ks')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "There are some important issues that one should be aware of in using the simple E-M algorithm discussed above: \n",
    "- The globally optimal result may never be achieved. As is the case with all optimisation routines, although the result is improving, it may not be moving to the globally optimal solution. \n",
    "- *k*-means is limited to linear cluster boundaries. The fact that *k*-means is finding samples as close as possible in cartesian space means that the clustering cannot have more complex geometries. \n",
    "- *k*-means can be slow for a large number of samples. Each iteration must access every point in the dataset (and in our implementation, it accesses each point `n_clusters` number of times)!\n",
    "- The number of clusters must be selected beforehand. We must have some {term}`a priori` knowledge about our dataset to effectively apply a *k*-means clustering.\n",
    "\n",
    "This final point is a common concern in *k*-means clustering, and other clustering algorithms. \n",
    "Therefore, let's look at a popular tool used to find the \"correct\" number of clusters. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "special-topics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
