{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gymnasium\n",
    "\n",
    "Before we further introduce reinforcement learning concepts, let's look at the Python package `gymnasium`. \n",
    "`gymnasium` is an open-source Python library for developing and comparing reinforcement learning algorithms.\n",
    "Initially developed by OpenAI, it is now maintained by a community of developers.  \n",
    "Like many other Python packages, the `gymnasium` package follows a specific structure. \n",
    "Let's start by creating one of the environments packaged with `gymnasium`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `gymnasium` package comes with a range of environments that we can work with. \n",
    "The `pprint_registry()` function shows a list of the available environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym.pprint_registry()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrate, we will use the `\"LunarLander-v3\"` environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v3\", render_mode='rgb_array')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we instantiated the environment with the `render_mode` of `'rgb_array'`; this is necessary to produce the animations below. \n",
    "\n",
    "The `env` object has a series of methods associated with providing actions to and generally controlling the environment. \n",
    "The first thing we can do is `reset` the environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation, info = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This returns an `observation` and some `info` (we can usually ignore the latter). \n",
    "The `observation` structure depends on the environment being used; for [the `LunarLander-v3` the documentation](https://gymnasium.farama.org/environments/box2d/lunar_lander/) tells us that the observation is an 8-dimensional vector containing: \n",
    "- coordinates in *x* and *y*, \n",
    "- velocities in *x* and *y*,\n",
    "- angle and angular velocity, \n",
    "- two boolean integers representing whether each leg is in contact with the ground. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environment works because an action is supplied each time the `step` method is called. \n",
    "Again, the action depends on the environment, with the `LunarLander-v3` having four possible actions: \n",
    "- `0`: do nothing\n",
    "- `1`: fire left orientation engine\n",
    "- `2`: fire main engine\n",
    "- `3`: fire right orientation engine\n",
    "\n",
    "Below, we select the action based on the modulo of the iteration number by four. \n",
    "This is run for 1000 steps or until termination. \n",
    "\n",
    "````{margin}\n",
    "```{note}\n",
    "The `reset` method here is given a `seed=0` to ensure the environment is appropriately initialised for the course book. \n",
    "I recommend removing that seed if you want to train a general solution. \n",
    "```\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "current_rewards = 0\n",
    "obs = env.reset(seed=0)[0]\n",
    "render = []\n",
    "for step in range(env.spec.max_episode_steps):\n",
    "    action = step % 4\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    current_rewards += reward\n",
    "    render.append(env.render())\n",
    "    if terminated:\n",
    "        break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then visualise the render with the following code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "plt.rcParams[\"animation.html\"] = \"jshtml\"\n",
    "plt.ioff()\n",
    "\n",
    "fig, ax = plt.subplots(1,1)\n",
    "\n",
    "def animate(t):\n",
    "    plt.cla()\n",
    "    ax.imshow(render[t] / 255)\n",
    "    ax.text(500, 50, f'Step: {t}', color='white')\n",
    "\n",
    "ani = matplotlib.animation.FuncAnimation(fig, animate, frames=len(render))\n",
    "html = HTML(ani.to_jshtml())\n",
    "display(html)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprisingly, this specific starting configuration manages to survive the landing with this simplistic action. \n",
    "Other starting configurations would be less lucky. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "special-topics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
