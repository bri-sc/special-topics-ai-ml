{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Search\n",
    "\n",
    "The training of a reinforcement learning loop is a process that aims to generate a policy that leads to the reward being maximised. \n",
    "This is an *optimisation* process; instead of optimising specific values, we are trying to optimise the policy defining the action. \n",
    "It is easiest to imagine this policy as a discrete variable to be optimised, i.e., should the policy be policy A or policy B. \n",
    "However, if we break the policy down into constituent parts, we can see the continuous nature of the problem. \n",
    "Consider the robot vacuum example; there are two parameters that can be controlled: how large the rotations of the vacuum are ($r$) and how frequently it does so ($p$). \n",
    "This is shown visually for a few positions in policy space in {numref}`vacuum`. \n",
    "\n",
    "```{figure} ./../images/policy-search.png\n",
    "---\n",
    "height: 300px\n",
    "align: vacuum\n",
    "---\n",
    "Some examples of policies may be found in the example of a robot vacuum. \n",
    "```\n",
    "\n",
    "Let's consider two possible policies for the `LunarLander-v3` environment. \n",
    "One will be the modulo example used before, and the other will be a more logical approach, where the lunar is moved left or right based on the position in the *x* dimension and the downward thruster is only used when the *y* velocity is greater than some threshold. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modulo_policy(step, obs):\n",
    "    \"\"\"\n",
    "    Returns and action based on the modulo of the step in the episode.\n",
    "    \n",
    "    :param step: The step in the episode.\n",
    "    :return: The action to take.\n",
    "    \"\"\"\n",
    "    return step % 4\n",
    "\n",
    "def logical_policy(step, obs):\n",
    "    \"\"\"\n",
    "    A more logical policy that takes into account the observation.\n",
    "    \n",
    "    :param obs: The observation.\n",
    "    :return: The action to take.\n",
    "    \"\"\"\n",
    "    x_pos = obs[0]\n",
    "    y_vel = obs[3]\n",
    "    if y_vel < -0.4:\n",
    "        return 2\n",
    "    elif x_pos < -0.1:\n",
    "        return 3\n",
    "    elif x_pos > 0.1:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now compare the two policies, over 500 episodes of the `LunarLander-v3`, to see which performs best. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make('LunarLander-v3', render_mode='rgb_array')\n",
    "\n",
    "policies = [modulo_policy, logical_policy]\n",
    "total_rewards = np.zeros((2, 500))\n",
    "render = [None, None]\n",
    "for i, policy in enumerate(policies):\n",
    "    for repeat in range(total_rewards.shape[1]):\n",
    "        current_rewards = 0\n",
    "        obs = env.reset()[0]\n",
    "        current_render = []\n",
    "        for step in range(env.spec.max_episode_steps):\n",
    "            action = policy(step, obs)\n",
    "            obs, reward, terminated, truncated, info = env.step(action)\n",
    "            current_rewards += reward\n",
    "            current_render.append(env.render())\n",
    "            if terminated:\n",
    "                break\n",
    "        if current_rewards > total_rewards[i].max():\n",
    "            render[i] = current_render\n",
    "        total_rewards[i, repeat] = current_rewards\n",
    "\n",
    "        env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now compare the total reward from each episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.hist(total_rewards[0], label='Modulo Policy', density=True)\n",
    "ax.hist(total_rewards[1], label='Logical Policy', density=True)\n",
    "ax.legend()\n",
    "ax.set_xlabel('Reward')\n",
    "ax.set_ylabel('p(Reward)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that the modulo policy, on average, does better than the logical policy in this case.\n",
    "However, the logical policy follows a bimodal distribution, so with some tuning, it could potentially outperform the modulo policy.\n",
    "\n",
    "We will save these rewards for use later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('total_rewards.txt', total_rewards)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "special-topics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
