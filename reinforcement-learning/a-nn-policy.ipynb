{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Policies\n",
    "\n",
    "Neural networks are highly effective for training reinforcement learning algorithms. \n",
    "Similar to other neural network examples, it can be hard to interpret the optimised network. \n",
    "Let's try to build a network to train for this `LunarLander-v3`. \n",
    "Our policy network will be a simple three-linear layer network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple feedforward neural network with 2 hidden layers.\n",
    "    \n",
    "    :param obs_dim: The dimension of the input obs.\n",
    "    :param action_dim: The dimension of the output action.\n",
    "    \"\"\"\n",
    "    def __init__(self, obs_dim, action_dim):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(obs_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, action_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the network.\n",
    "        \n",
    "        :param x: The input obs.\n",
    "        :return: The output action.\n",
    "        \"\"\"\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the broader training strategy will involve using a deep *Q*-network. \n",
    "This approach uses a Replay Buffer, which reduces the correlation between episodes while enabling the reuse of past experiences. \n",
    "For more on this structure, there is a good [Stack Exchange answer](https://datascience.stackexchange.com/questions/20535/what-is-experience-replay-and-what-are-its-benefits) on the subject. \n",
    "We construct below the Replay Buffer class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import deque\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"\n",
    "    A simple replay buffer for storing experiences.\n",
    "    \n",
    "    :param capacity: The maximum capacity of the buffer.\n",
    "    \"\"\"\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, obs, action, reward, next_obs, done):\n",
    "        \"\"\"\n",
    "        Push an experience to the buffer.\n",
    "        \n",
    "        :param obs: The current obs.\n",
    "        :param action: The action taken.\n",
    "        :param reward: The reward received.\n",
    "        :param next_obs: The next obs.\n",
    "        :param done: Whether the episode is done.\n",
    "        \"\"\"\n",
    "        self.buffer.append((obs, action, reward, next_obs, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"\n",
    "        Sample a batch of experiences from the buffer.\n",
    "        \n",
    "        :param batch_size: The size of the batch.\n",
    "        :return: The batch of experiences.\n",
    "        \"\"\"\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        :return: The length of the buffer.\n",
    "        \"\"\"\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final stage is to build the training loop. \n",
    "This is where the *Q*-learning component comes in. \n",
    "We can think of *Q*-learning as a table that stores the best learning actions, where each cell holds a *Q*-value and estimates how good that action is for a given state of the environment. \n",
    "The *Q*-values are then updated iteratively using the Bellman equation, \n",
    "\n",
    "$$\n",
    "Q(s, a) = Q(s, a) + \\alpha\\left[r + \\gamma \\max Q(s', a') - Q(s, a)\\right], \n",
    "$$\n",
    "\n",
    "where $s$ and $s'$ are the current and next observations, $a$ and $a'$ are the current and next actions, $r$ is the reward received for the next action, $\\alpha$ is the learning rate, and $\\gamma is the discount factor, which controls how much future rewards matter. \n",
    "Here, we use a deep *Q*-network, so the neural network estimates them instead of explicitly computing the *Q*-values.\n",
    "To enable this, we create two networks, the `policy_net` and the `target_net`, which provide the *Q*values for the next observation.\n",
    "\n",
    "To add some randomness to the network, we have a Monte Carlo step utilised in a simulated annealing fashion. \n",
    "This means the likelihood that the Monte Carlo (randomly selected) policy is used decreases as the training progresses. \n",
    "The `EPSILON_DECAY` hyperparameter controls the amount that this decreases. \n",
    "````{margin}\n",
    "```{note}\n",
    "Please read through this code carefully and ensure you understand it. \n",
    "```\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "GAMMA = 0.99\n",
    "LEARNING_RATE = 1e-3\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 10000\n",
    "EPSILON_DECAY = 0.995\n",
    "MIN_EPSILON = 0.01\n",
    "TARGET_UPDATE = 10\n",
    "\n",
    "def train(env, episodes=1000):\n",
    "    obs_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "    \n",
    "    policy_net = QNetwork(obs_dim, action_dim)\n",
    "    target_net = QNetwork(obs_dim, action_dim)\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    target_net.eval()\n",
    "    \n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=LEARNING_RATE)\n",
    "    replay_buffer = ReplayBuffer(BUFFER_SIZE)\n",
    "    epsilon = np.ones(episodes)\n",
    "    total_reward = np.zeros(episodes)\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        obs, _ = env.reset()\n",
    "        obs = torch.tensor(obs, dtype=torch.float32).unsqueeze(0)\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            if random.random() < epsilon[episode]:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    action = policy_net(obs).argmax().item()\n",
    "            \n",
    "            next_obs, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            next_obs = torch.tensor(next_obs, dtype=torch.float32).unsqueeze(0)\n",
    "            replay_buffer.push(obs, action, reward, next_obs, done)\n",
    "            \n",
    "            obs = next_obs\n",
    "            total_reward[episode] += reward\n",
    "            \n",
    "            if len(replay_buffer) >= BATCH_SIZE:\n",
    "                batch = replay_buffer.sample(BATCH_SIZE)\n",
    "                obss, actions, rewards, next_obss, dones = zip(*batch)\n",
    "                \n",
    "                obss = torch.cat(obss)\n",
    "                actions = torch.tensor(actions, dtype=torch.int64).unsqueeze(1)\n",
    "                rewards = torch.tensor(rewards, dtype=torch.float32).unsqueeze(1)\n",
    "                next_obss = torch.cat(next_obss)\n",
    "                dones = torch.tensor(dones, dtype=torch.float32).unsqueeze(1)\n",
    "                \n",
    "                q_values = policy_net(obss).gather(1, actions)\n",
    "                next_q_values = target_net(next_obss).max(1, keepdim=True)[0]\n",
    "                target_q_values = rewards + GAMMA * next_q_values * (1 - dones)\n",
    "                \n",
    "                loss = F.mse_loss(q_values, target_q_values.detach())\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        \n",
    "        if episode % TARGET_UPDATE == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "        \n",
    "        if episode < episodes - 1:\n",
    "            epsilon[episode+1] = max(MIN_EPSILON, epsilon[episode] * EPSILON_DECAY)\n",
    "    \n",
    "    return policy_net, total_reward, epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now train the network to over 1000 episodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "env = gym.make('LunarLander-v3', render_mode='rgb_array')\n",
    "trained_policy, nn_training_rewards, epsilon = train(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And plot the reward trend as a function of the episode. \n",
    "Note the slight upward trend over the training time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(nn_training_rewards)\n",
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylabel('Total Reward')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use this trained policy over another 500 random episodes to see how it compares to the modulo and logical policies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_rewards = np.zeros((500))\n",
    "render = None\n",
    "for repeat in range(nn_rewards.shape[0]):\n",
    "    current_rewards = 0\n",
    "    obs = env.reset()[0]\n",
    "    current_render = []\n",
    "    for step in range(env.spec.max_episode_steps):\n",
    "        action = trained_policy(torch.tensor(obs, dtype=torch.float32).unsqueeze(0)).argmax().item()\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        current_rewards += reward\n",
    "        current_render.append(env.render())\n",
    "        if terminated:\n",
    "            break\n",
    "    if current_rewards > nn_rewards.max():\n",
    "        render = current_render\n",
    "    nn_rewards[repeat] = current_rewards\n",
    "\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_rewards = np.loadtxt('total_rewards.txt')\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.hist(total_rewards[0], label='Modulo Policy', density=True)\n",
    "ax.hist(total_rewards[1], label='Logical Policy', density=True)\n",
    "ax.hist(nn_rewards, label='Neural Network Policy', density=True)\n",
    "ax.legend()\n",
    "ax.set_xlabel('Reward')\n",
    "ax.set_ylabel('p(Reward)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that with no hyperparameter optimisation, the neural network policy is, on average, outperforming the other two approaches. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "special-topics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
