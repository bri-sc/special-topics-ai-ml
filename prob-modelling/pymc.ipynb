{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using PyMC\n",
    "\n",
    "[PyMC](https://www.pymc.io) is a very powerful Python library designed for probabilistic and Bayesian analysis. \n",
    "Here, we show that PyMC can be used to perform the same likelihood sampling that we previously wrote our own algorithm for. \n",
    "\n",
    "Below, we read in the data and build the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "data = pd.read_csv('../data/first-order.csv')\n",
    "\n",
    "D = [norm(data['At'][i], data['At_err'][i]) for i in range(len(data))]\n",
    "\n",
    "def first_order(t, k, A0):\n",
    "    \"\"\"\n",
    "    A first order rate equation.\n",
    "    \n",
    "    :param t: The time to evaluate the rate equation at.\n",
    "    :param k: The rate constant.\n",
    "    :param A0: The initial concentration of A.\n",
    "    \n",
    "    :return: The concentration of A at time t.\n",
    "    \"\"\"\n",
    "    return A0 * np.exp(-k * t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to construct the PyMC sampler. \n",
    "The format that PyMC expects can be a bit unfamiliar. \n",
    "\n",
    "First we create objects for the two parameters, these are bounded so $0 \\leq k < 1$ and $0 \\leq [A]_0 < 10$.\n",
    "Strictly, these are [prior probabilities](./priors.md), which we will look at next, but using uniform distributions means this is mathematically equivalent to likelihood sampling.\n",
    "Next, we create a normally distributed likelihood function to compare the data and the model. \n",
    "Finally, we sample for 1000 steps, with 10 chains. \n",
    "The `tune` parameter is the number of steps for tuning the Markov chain step sizes. \n",
    "````{margin}\n",
    "```{note}\n",
    "Other parameters for the `pm.sample` method can be found in the [appropriate documentation](https://www.pymc.io/projects/docs/en/stable/api/generated/pymc.sample.html).\n",
    "```\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc as pm\n",
    "\n",
    "with pm.Model() as model:\n",
    "    k = pm.Uniform('k', 0, 1)\n",
    "    A0 = pm.Uniform('A0', 0, 10)\n",
    "    \n",
    "    At = pm.Normal('At', \n",
    "                   mu=first_order(data['t'], k, A0), \n",
    "                   sigma=data['At_err'], \n",
    "                   observed=data['At'])\n",
    "    \n",
    "    trace = pm.sample(1000, tune=1000, chains=10, progressbar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike the code that we created previously, PyMC defaults to using the NUTS sampler, which stands for No-U-Turn sampler {cite}`Hoffman2014`. \n",
    "This sampler enables the step size tuning that we have taken advantage of. \n",
    "\n",
    "This results in a object assigned to the variable `trace`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This contains the chain information amoung other things. \n",
    "Instead of probing into the `trace` object, we can take advantage of functionality from the `arviz` library to produce some informative plots. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import arviz as az\n",
    "\n",
    "az.plot_trace(trace, var_names=[\"k\", \"A0\"])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we can see the trace of each of the different chains. \n",
    "The chains appear to have converged to the same distribution.\n",
    "We can get the flat chains with the following function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_chain = np.vstack([trace.posterior['k'].values.flatten(), trace.posterior['A0'].values.flatten()]).T\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "chains_df = pd.DataFrame(flat_chain, columns=['k', 'A0'])\n",
    "sns.jointplot(data=chains_df, x='k', y='A0', kind='kde')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear that, using PyMC, we have much better sampling of the distributions. \n",
    "This makes using summary statistics, like the mean and standard deviation much more reliable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.summary(trace, var_names=[\"k\", \"A0\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "special-topics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
