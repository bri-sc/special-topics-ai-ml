{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing the Posterior\n",
    "\n",
    "Previously, we discussed how to compute the likelihood and the probability of observing some data given a model and parameters, and we have now seen how to propose a prior probability. \n",
    "Therefore, if we look at Bayes' theorem again, we can start to look at the so-called posterior. \n",
    "\n",
    "$$\n",
    "p(A | B) = \\frac{p(B | A) p(A)}{p(B)},\n",
    "$$\n",
    "\n",
    "the posterior is $p(A | B)$, the conditional probability that we are trying to compute. \n",
    "However, we have not yet covered how to compute Bayes' theorem's denominator, known as the *evidence* term.\n",
    "\n",
    "## Sampling Parameters\n",
    "\n",
    "If we rewrite Bayes' theorem in the notation of our data, $D$, model, $M$, and parameters, $\\Theta$, we get the following relation,\n",
    "\n",
    "$$\n",
    "p[M(\\Theta) | D] = \\frac{p[D | M(\\Theta)] p[M(\\Theta)]}{p(D)}.\n",
    "$$ (bayes)\n",
    "\n",
    "Our Bayesian analysis with the above equation usually aims to understand the distribution of the parameters, $\\Theta$. \n",
    "We note that $\\Theta$ does not appear in the denominator; therefore, changing the value of $\\Theta$ does not affect this. \n",
    "This makes the denominator simply a scaling factor, so we rewrite Eqn. {eq}`bayes` as, \n",
    "\n",
    "$$\n",
    "p[M(\\Theta) | D] \\propto p[D | M(\\Theta)] p[M(\\Theta)].\n",
    "$$ \n",
    "\n",
    "````{margin}\n",
    "```{note}\n",
    "Although it is not necessary at this stage, we will look at how the denominator is determined. \n",
    "```\n",
    "````\n",
    "Therefore, in sampling and maximising $p[M(\\Theta) | D]$, it is not necessary to compute the denominator.\n",
    "We can now look at using PyMC to achieve this. \n",
    "Let's load in the chemical data we have seen previously. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "data = pd.read_csv('../data/first-order.csv')\n",
    "\n",
    "D = [norm(data['At'][i], data['At_err'][i]) for i in range(len(data))]\n",
    "\n",
    "def first_order(t, k, A0):\n",
    "    \"\"\"\n",
    "    A first order rate equation.\n",
    "    \n",
    "    :param t: The time to evaluate the rate equation at.\n",
    "    :param k: The rate constant.\n",
    "    :param A0: The initial concentration of A.\n",
    "    \n",
    "    :return: The concentration of A at time t.\n",
    "    \"\"\"\n",
    "    return A0 * np.exp(-k * t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only change necessary is to use the informative prior in the `pm.Model`. \n",
    "This is because PyMC always performs posterior sampling; it is just that the use of an uninformative prior (the uniform distribution) is numerically equivalent to sampling just the likelihood. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc as pm\n",
    "\n",
    "with pm.Model() as model:\n",
    "    k = pm.Normal('k', 0.15, 0.01)\n",
    "    A0 = pm.Normal('A0', 7.5, 0.5)\n",
    "    \n",
    "    At = pm.Normal('At', \n",
    "                   mu=first_order(data['t'], k, A0), \n",
    "                   sigma=data['At_err'], \n",
    "                   observed=data['At'])\n",
    "    \n",
    "    trace = pm.sample(1000, tune=1000, chains=10, progressbar=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we can visualise the traces using `arviz`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import arviz as az\n",
    "\n",
    "az.plot_trace(trace, var_names=[\"k\", \"A0\"])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By comparing this to the likelihood sampling result, we can see the influence of the priors and Bayes' theorem.\n",
    "It is important to highlight that we are computing the probability of the model, Eqn, {eq}`rate` here, with the parameters above given the data, instead of the probability that data, $D$, would be observed given some model, which was what was computed in the likelihood sampling. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "special-topics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
